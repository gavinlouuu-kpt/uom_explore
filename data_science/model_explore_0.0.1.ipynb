{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\code\\uom_explore\\.env\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>baseline_140</th>\n",
       "      <th>baseline_150</th>\n",
       "      <th>baseline_152</th>\n",
       "      <th>baseline_155</th>\n",
       "      <th>baseline_157</th>\n",
       "      <th>baseline_160</th>\n",
       "      <th>baseline_162</th>\n",
       "      <th>baseline_165</th>\n",
       "      <th>baseline_167</th>\n",
       "      <th>...</th>\n",
       "      <th>responsivity_227</th>\n",
       "      <th>responsivity_230</th>\n",
       "      <th>responsivity_232</th>\n",
       "      <th>responsivity_235</th>\n",
       "      <th>responsivity_237</th>\n",
       "      <th>responsivity_240</th>\n",
       "      <th>responsivity_242</th>\n",
       "      <th>responsivity_245</th>\n",
       "      <th>responsivity_247</th>\n",
       "      <th>responsivity_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9813.187476</td>\n",
       "      <td>9645.462062</td>\n",
       "      <td>9801.200273</td>\n",
       "      <td>10667.638990</td>\n",
       "      <td>11810.233304</td>\n",
       "      <td>12862.940343</td>\n",
       "      <td>13729.325616</td>\n",
       "      <td>14920.159529</td>\n",
       "      <td>16340.258019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>0.205917</td>\n",
       "      <td>0.209839</td>\n",
       "      <td>0.195255</td>\n",
       "      <td>0.194283</td>\n",
       "      <td>0.186699</td>\n",
       "      <td>0.178084</td>\n",
       "      <td>0.176998</td>\n",
       "      <td>0.160773</td>\n",
       "      <td>0.155035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11071.299277</td>\n",
       "      <td>11955.695353</td>\n",
       "      <td>14050.930337</td>\n",
       "      <td>17267.145138</td>\n",
       "      <td>20601.106281</td>\n",
       "      <td>24583.195449</td>\n",
       "      <td>26983.461079</td>\n",
       "      <td>29634.560412</td>\n",
       "      <td>31413.192823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.037694</td>\n",
       "      <td>0.038184</td>\n",
       "      <td>0.040298</td>\n",
       "      <td>0.035455</td>\n",
       "      <td>0.036144</td>\n",
       "      <td>0.035012</td>\n",
       "      <td>0.031656</td>\n",
       "      <td>0.030211</td>\n",
       "      <td>0.032798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13250.917865</td>\n",
       "      <td>14000.780949</td>\n",
       "      <td>16696.731867</td>\n",
       "      <td>21067.945739</td>\n",
       "      <td>26216.702246</td>\n",
       "      <td>30906.307231</td>\n",
       "      <td>34484.682387</td>\n",
       "      <td>37199.194526</td>\n",
       "      <td>40262.371268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046951</td>\n",
       "      <td>0.048203</td>\n",
       "      <td>0.042090</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.041988</td>\n",
       "      <td>0.039039</td>\n",
       "      <td>0.046367</td>\n",
       "      <td>0.039396</td>\n",
       "      <td>0.039468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15102.438187</td>\n",
       "      <td>16004.286060</td>\n",
       "      <td>19645.107534</td>\n",
       "      <td>25639.237126</td>\n",
       "      <td>31823.493209</td>\n",
       "      <td>38074.486698</td>\n",
       "      <td>42671.679031</td>\n",
       "      <td>46122.577602</td>\n",
       "      <td>49493.894335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083644</td>\n",
       "      <td>0.072952</td>\n",
       "      <td>0.078996</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.078357</td>\n",
       "      <td>0.076305</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.076581</td>\n",
       "      <td>0.071941</td>\n",
       "      <td>0.073366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16524.114584</td>\n",
       "      <td>17757.437340</td>\n",
       "      <td>21865.425307</td>\n",
       "      <td>27946.605526</td>\n",
       "      <td>34718.581431</td>\n",
       "      <td>42623.762495</td>\n",
       "      <td>46876.221361</td>\n",
       "      <td>52782.714175</td>\n",
       "      <td>54820.415879</td>\n",
       "      <td>...</td>\n",
       "      <td>2.045861</td>\n",
       "      <td>2.045302</td>\n",
       "      <td>2.054864</td>\n",
       "      <td>2.082283</td>\n",
       "      <td>2.069626</td>\n",
       "      <td>2.076433</td>\n",
       "      <td>2.051532</td>\n",
       "      <td>2.047403</td>\n",
       "      <td>2.061066</td>\n",
       "      <td>2.038500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   channel_id  baseline_140  baseline_150  baseline_152  baseline_155  \\\n",
       "0           0   9813.187476   9645.462062   9801.200273  10667.638990   \n",
       "1           1  11071.299277  11955.695353  14050.930337  17267.145138   \n",
       "2           2  13250.917865  14000.780949  16696.731867  21067.945739   \n",
       "3           3  15102.438187  16004.286060  19645.107534  25639.237126   \n",
       "4           4  16524.114584  17757.437340  21865.425307  27946.605526   \n",
       "\n",
       "   baseline_157  baseline_160  baseline_162  baseline_165  baseline_167  ...  \\\n",
       "0  11810.233304  12862.940343  13729.325616  14920.159529  16340.258019  ...   \n",
       "1  20601.106281  24583.195449  26983.461079  29634.560412  31413.192823  ...   \n",
       "2  26216.702246  30906.307231  34484.682387  37199.194526  40262.371268  ...   \n",
       "3  31823.493209  38074.486698  42671.679031  46122.577602  49493.894335  ...   \n",
       "4  34718.581431  42623.762495  46876.221361  52782.714175  54820.415879  ...   \n",
       "\n",
       "   responsivity_227  responsivity_230  responsivity_232  responsivity_235  \\\n",
       "0          0.213747          0.205917          0.209839          0.195255   \n",
       "1          0.033074          0.037694          0.038184          0.040298   \n",
       "2          0.046951          0.048203          0.042090          0.033619   \n",
       "3          0.083644          0.072952          0.078996          0.079489   \n",
       "4          2.045861          2.045302          2.054864          2.082283   \n",
       "\n",
       "   responsivity_237  responsivity_240  responsivity_242  responsivity_245  \\\n",
       "0          0.194283          0.186699          0.178084          0.176998   \n",
       "1          0.035455          0.036144          0.035012          0.031656   \n",
       "2          0.040335          0.041988          0.039039          0.046367   \n",
       "3          0.078357          0.076305          0.075400          0.076581   \n",
       "4          2.069626          2.076433          2.051532          2.047403   \n",
       "\n",
       "   responsivity_247  responsivity_250  \n",
       "0          0.160773          0.155035  \n",
       "1          0.030211          0.032798  \n",
       "2          0.039396          0.039468  \n",
       "3          0.071941          0.073366  \n",
       "4          2.061066          2.038500  \n",
       "\n",
       "[5 rows x 124 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spk_data = \"D:\\\\code\\\\uom_explore\\\\model_input\\\\3_features.csv\"\n",
    "\n",
    "hkr_wsl_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_matrix.csv\"\n",
    "hkr_pca_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_pca.csv\"\n",
    "\n",
    "spk_json = \"/home/gavinlouuu/coding/uom_explore/data_science/parameter.json\"\n",
    "hkr_wsl_json = \"/home/hk-wsl/code/uom_explore/data_science/parameter.json\"\n",
    "\n",
    "data_path = spk_data\n",
    "param_path = hkr_wsl_json\n",
    "\n",
    "with open('parameter.json','r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# drop experiment_id column\n",
    "df.drop('experiment_id', axis=1, inplace=True)\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data for NaN\n",
    "# df.isnull().sum()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove channel_id from the list\n",
    "\n",
    "# Hyperparameters\n",
    "# Extract parameters from the JSON object\n",
    "hidden_size = params['hidden_size']\n",
    "ground_truth = params['ground_truth']\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "learning_rate = params['learning_rate']\n",
    "momentum_value = params['momentum_value']\n",
    "dropout_rate = params['dropout']\n",
    "\n",
    "# features_raw = params['full_features']\n",
    "# Convert feature list to strings\n",
    "# features = list(map(str, features_raw))\n",
    "# Get header list and remove 'channel_id'\n",
    "header = df.columns.values.tolist()\n",
    "header.remove('channel_id')\n",
    "\n",
    "# Select features and remove ground_truth from feature list\n",
    "features = [col for col in header if col != ground_truth]\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(ground_truth, axis=1)\n",
    "# select features to be used\n",
    "X = X[features]\n",
    "\n",
    "input_size = len(X.columns)  # removing the ground truth from the number of columns counted\n",
    "num_classes = df[ground_truth].nunique()\n",
    "\n",
    "y = df[ground_truth]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # This makes 60%, 20%, 20%\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(0,255)) # \n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_scaled, y_train)\n",
    "val_dataset = TensorDataset(X_val_scaled, y_val)\n",
    "test_dataset = TensorDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.6079, Validation Accuracy: 0.2526\n",
      "Epoch 2, Validation Loss: 1.5990, Validation Accuracy: 0.4670\n",
      "Epoch 3, Validation Loss: 1.5815, Validation Accuracy: 0.6693\n",
      "Epoch 4, Validation Loss: 1.5534, Validation Accuracy: 0.7500\n",
      "Epoch 5, Validation Loss: 1.5161, Validation Accuracy: 0.8325\n",
      "Epoch 6, Validation Loss: 1.4767, Validation Accuracy: 0.8620\n",
      "Epoch 7, Validation Loss: 1.4397, Validation Accuracy: 0.8602\n",
      "Epoch 8, Validation Loss: 1.4051, Validation Accuracy: 0.8446\n",
      "Epoch 9, Validation Loss: 1.3754, Validation Accuracy: 0.8507\n",
      "Epoch 10, Validation Loss: 1.3516, Validation Accuracy: 0.8507\n",
      "Epoch 11, Validation Loss: 1.3339, Validation Accuracy: 0.8446\n",
      "Epoch 12, Validation Loss: 1.3112, Validation Accuracy: 0.8290\n",
      "Epoch 13, Validation Loss: 1.2960, Validation Accuracy: 0.8368\n",
      "Epoch 14, Validation Loss: 1.2798, Validation Accuracy: 0.8290\n",
      "Epoch 15, Validation Loss: 1.2690, Validation Accuracy: 0.8368\n",
      "Epoch 16, Validation Loss: 1.2586, Validation Accuracy: 0.8368\n",
      "Epoch 17, Validation Loss: 1.2486, Validation Accuracy: 0.8446\n",
      "Epoch 18, Validation Loss: 1.2384, Validation Accuracy: 0.8368\n",
      "Epoch 19, Validation Loss: 1.2277, Validation Accuracy: 0.8290\n",
      "Epoch 20, Validation Loss: 1.2166, Validation Accuracy: 0.8290\n",
      "Epoch 21, Validation Loss: 1.2096, Validation Accuracy: 0.8368\n",
      "Epoch 22, Validation Loss: 1.2018, Validation Accuracy: 0.8368\n",
      "Epoch 23, Validation Loss: 1.1954, Validation Accuracy: 0.8368\n",
      "Epoch 24, Validation Loss: 1.1897, Validation Accuracy: 0.8368\n",
      "Epoch 25, Validation Loss: 1.1839, Validation Accuracy: 0.8368\n",
      "Epoch 26, Validation Loss: 1.1794, Validation Accuracy: 0.8446\n",
      "Epoch 27, Validation Loss: 1.1721, Validation Accuracy: 0.8368\n",
      "Epoch 28, Validation Loss: 1.1655, Validation Accuracy: 0.8446\n",
      "Epoch 29, Validation Loss: 1.1582, Validation Accuracy: 0.8368\n",
      "Epoch 30, Validation Loss: 1.1540, Validation Accuracy: 0.8446\n",
      "Epoch 31, Validation Loss: 1.1481, Validation Accuracy: 0.8446\n",
      "Epoch 32, Validation Loss: 1.1460, Validation Accuracy: 0.8446\n",
      "Epoch 33, Validation Loss: 1.1427, Validation Accuracy: 0.8663\n",
      "Epoch 34, Validation Loss: 1.1380, Validation Accuracy: 0.8524\n",
      "Epoch 35, Validation Loss: 1.1359, Validation Accuracy: 0.8585\n",
      "Epoch 36, Validation Loss: 1.1324, Validation Accuracy: 0.8663\n",
      "Epoch 37, Validation Loss: 1.1293, Validation Accuracy: 0.8663\n",
      "Epoch 38, Validation Loss: 1.1264, Validation Accuracy: 0.8741\n",
      "Epoch 39, Validation Loss: 1.1228, Validation Accuracy: 0.8524\n",
      "Epoch 40, Validation Loss: 1.1212, Validation Accuracy: 0.8741\n",
      "Epoch 41, Validation Loss: 1.1192, Validation Accuracy: 0.8741\n",
      "Epoch 42, Validation Loss: 1.1186, Validation Accuracy: 0.8741\n",
      "Epoch 43, Validation Loss: 1.1156, Validation Accuracy: 0.8741\n",
      "Epoch 44, Validation Loss: 1.1134, Validation Accuracy: 0.8819\n",
      "Epoch 45, Validation Loss: 1.1090, Validation Accuracy: 0.8741\n",
      "Epoch 46, Validation Loss: 1.1102, Validation Accuracy: 0.8741\n",
      "Epoch 47, Validation Loss: 1.1043, Validation Accuracy: 0.8819\n",
      "Epoch 48, Validation Loss: 1.1011, Validation Accuracy: 0.8819\n",
      "Epoch 49, Validation Loss: 1.0981, Validation Accuracy: 0.8819\n",
      "Epoch 50, Validation Loss: 1.0949, Validation Accuracy: 0.8819\n",
      "Epoch 51, Validation Loss: 1.0966, Validation Accuracy: 0.8819\n",
      "Epoch 52, Validation Loss: 1.0953, Validation Accuracy: 0.8819\n",
      "Epoch 53, Validation Loss: 1.0945, Validation Accuracy: 0.8958\n",
      "Epoch 54, Validation Loss: 1.0897, Validation Accuracy: 0.8958\n",
      "Epoch 55, Validation Loss: 1.0879, Validation Accuracy: 0.9036\n",
      "Epoch 56, Validation Loss: 1.0847, Validation Accuracy: 0.9036\n",
      "Epoch 57, Validation Loss: 1.0803, Validation Accuracy: 0.9036\n",
      "Epoch 58, Validation Loss: 1.0734, Validation Accuracy: 0.9097\n",
      "Epoch 59, Validation Loss: 1.0725, Validation Accuracy: 0.9097\n",
      "Epoch 60, Validation Loss: 1.0725, Validation Accuracy: 0.9097\n",
      "Epoch 61, Validation Loss: 1.0733, Validation Accuracy: 0.9115\n",
      "Epoch 62, Validation Loss: 1.0710, Validation Accuracy: 0.9253\n",
      "Epoch 63, Validation Loss: 1.0666, Validation Accuracy: 0.9332\n",
      "Epoch 64, Validation Loss: 1.0617, Validation Accuracy: 0.9253\n",
      "Epoch 65, Validation Loss: 1.0607, Validation Accuracy: 0.9332\n",
      "Epoch 66, Validation Loss: 1.0545, Validation Accuracy: 0.9175\n",
      "Epoch 67, Validation Loss: 1.0516, Validation Accuracy: 0.9097\n",
      "Epoch 68, Validation Loss: 1.0529, Validation Accuracy: 0.9253\n",
      "Epoch 69, Validation Loss: 1.0535, Validation Accuracy: 0.9271\n",
      "Epoch 70, Validation Loss: 1.0488, Validation Accuracy: 0.9349\n",
      "Epoch 71, Validation Loss: 1.0416, Validation Accuracy: 0.9488\n",
      "Epoch 72, Validation Loss: 1.0386, Validation Accuracy: 0.9566\n",
      "Epoch 73, Validation Loss: 1.0355, Validation Accuracy: 0.9488\n",
      "Epoch 74, Validation Loss: 1.0344, Validation Accuracy: 0.9410\n",
      "Epoch 75, Validation Loss: 1.0345, Validation Accuracy: 0.9488\n",
      "Epoch 76, Validation Loss: 1.0420, Validation Accuracy: 0.9349\n",
      "Epoch 77, Validation Loss: 1.0366, Validation Accuracy: 0.9427\n",
      "Epoch 78, Validation Loss: 1.0260, Validation Accuracy: 0.9427\n",
      "Epoch 79, Validation Loss: 1.0198, Validation Accuracy: 0.9488\n",
      "Epoch 80, Validation Loss: 1.0189, Validation Accuracy: 0.9488\n",
      "Epoch 81, Validation Loss: 1.0200, Validation Accuracy: 0.9488\n",
      "Epoch 82, Validation Loss: 1.0245, Validation Accuracy: 0.9349\n",
      "Epoch 83, Validation Loss: 1.0210, Validation Accuracy: 0.9349\n",
      "Epoch 84, Validation Loss: 1.0182, Validation Accuracy: 0.9488\n",
      "Epoch 85, Validation Loss: 1.0180, Validation Accuracy: 0.9349\n",
      "Epoch 86, Validation Loss: 1.0171, Validation Accuracy: 0.9349\n",
      "Epoch 87, Validation Loss: 1.0170, Validation Accuracy: 0.9349\n",
      "Epoch 88, Validation Loss: 1.0116, Validation Accuracy: 0.9349\n",
      "Epoch 89, Validation Loss: 1.0164, Validation Accuracy: 0.9349\n",
      "Epoch 90, Validation Loss: 1.0148, Validation Accuracy: 0.9427\n",
      "Epoch 91, Validation Loss: 1.0084, Validation Accuracy: 0.9488\n",
      "Epoch 92, Validation Loss: 1.0082, Validation Accuracy: 0.9410\n",
      "Epoch 93, Validation Loss: 1.0081, Validation Accuracy: 0.9410\n",
      "Epoch 94, Validation Loss: 1.0118, Validation Accuracy: 0.9349\n",
      "Epoch 95, Validation Loss: 1.0095, Validation Accuracy: 0.9349\n",
      "Epoch 96, Validation Loss: 1.0113, Validation Accuracy: 0.9349\n",
      "Epoch 97, Validation Loss: 1.0136, Validation Accuracy: 0.9427\n",
      "Epoch 98, Validation Loss: 1.0019, Validation Accuracy: 0.9427\n",
      "Epoch 99, Validation Loss: 1.0029, Validation Accuracy: 0.9427\n",
      "Epoch 100, Validation Loss: 1.0032, Validation Accuracy: 0.9427\n"
     ]
    }
   ],
   "source": [
    "# # Define the MLP model\n",
    "    \n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_prob):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Softmax activation for the output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "            x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.layers[-1](x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPClassifier(input_size, hidden_size, num_classes, dropout_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "# Function to predict the class of new data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        _, predicted_class = torch.max(output, dim=1)\n",
    "    return predicted_class\n",
    "\n",
    "# Function to compute the accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred, dim=1)  # Get the index of the max log-probability\n",
    "    correct = (predicted == y_true).float().sum()\n",
    "    return correct / y_true.shape[0]\n",
    "\n",
    "# Training and evaluation loop\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # scheduler.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss += criterion(y_val_pred, y_val).item()\n",
    "                val_accuracy += calculate_accuracy(y_val_pred, y_val)\n",
    "                _, predicted_classes = torch.max(y_val_pred, dim=1)\n",
    "                # print(f'Predicted: {predicted_classes}, Actual: {y_val}') # actual predicted values\n",
    "        \n",
    "        \n",
    "        # Average the loss and accuracy over all validation batches\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Assuming the rest of your setup (model initialization, data loaders, etc.) is already done\n",
    "# Now you would just call train_and_evaluate\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# in_channels = params['in_channels']\n",
    "# out_channels = params['out_channels']\n",
    "# kernel_sizes = params['kernel_sizes']\n",
    "\n",
    "# class CNN1DClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_sizes, num_classes, dropout_rate=0.5):\n",
    "#         super(CNN1DClassifier, self).__init__()\n",
    "        \n",
    "#         assert len(out_channels) == len(kernel_sizes), \"The length of out_channels and kernel_sizes must be the same\"\n",
    "        \n",
    "#         self.convs = nn.ModuleList()\n",
    "#         self.bns = nn.ModuleList()  # Adding batch normalization if needed\n",
    "        \n",
    "#         current_in_channels = in_channels\n",
    "        \n",
    "#         for out_channel, kernel_size in zip(out_channels, kernel_sizes):\n",
    "#             self.convs.append(nn.Conv1d(current_in_channels, out_channel, kernel_size=kernel_size, stride=1, padding=kernel_size // 2))\n",
    "#             self.bns.append(nn.BatchNorm1d(out_channel))  # Optional: Add batch normalization\n",
    "#             current_in_channels = out_channel\n",
    "        \n",
    "#         # Calculate the size after all convolutional and pooling layers\n",
    "#         conv_output_size = input_size\n",
    "#         for kernel_size in kernel_sizes:\n",
    "#             conv_output_size = (conv_output_size + 2 * (kernel_size // 2) - (kernel_size - 1) - 1) // 1 + 1\n",
    "#             conv_output_size = conv_output_size // 2  # After pooling\n",
    "        \n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(out_channels[-1] * conv_output_size, 128)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'Input shape: {x.shape}')\n",
    "#         for conv, bn in zip(self.convs, self.bns):\n",
    "#             x = self.pool(torch.relu(bn(conv(x))))\n",
    "#             # print(f'After conv and pool: {x.shape}')\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         # print(f'After flatten: {x.shape}')\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # print(f'After fc1: {x.shape}')\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(f'After fc2: {x.shape}')\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# # Train the 1D CNN model\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, device='cpu'):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "#         print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "#         print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_corrects = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "#                 val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         val_loss = val_loss / len(val_loader.dataset)\n",
    "#         val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "#         print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = CNN1DClassifier(in_channels, out_channels, kernel_sizes, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# test_corrects = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         test_loss += loss.item() * inputs.size(0)\n",
    "#         test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# test_loss = test_loss / len(test_loader.dataset)\n",
    "# test_acc = test_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
