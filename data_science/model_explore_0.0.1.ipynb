{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update log\n",
    "2024/07/29\n",
    "- model accuracy increased to over 90% with more data points as feature\n",
    "- should incorporate optuna for visualisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.297113</td>\n",
       "      <td>-36.028848</td>\n",
       "      <td>-0.609223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.935828</td>\n",
       "      <td>-33.246099</td>\n",
       "      <td>-1.113492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.812983</td>\n",
       "      <td>-29.732352</td>\n",
       "      <td>-1.708627</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.757883</td>\n",
       "      <td>-26.591866</td>\n",
       "      <td>-1.720893</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.882516</td>\n",
       "      <td>-22.280264</td>\n",
       "      <td>0.086122</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PC1        PC2       PC3  channel_id\n",
       "0  -9.297113 -36.028848 -0.609223           0\n",
       "1  -7.935828 -33.246099 -1.113492           1\n",
       "2  -6.812983 -29.732352 -1.708627           2\n",
       "3  -5.757883 -26.591866 -1.720893           3\n",
       "4 -17.882516 -22.280264  0.086122           4"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spk_data = \"D:\\\\code\\\\uom_explore\\\\model_input\\\\3_features.csv\"\n",
    "spk_pca_data = \"D:\\\\code\\\\uom_explore\\\\model_input\\\\pca_df.csv\"\n",
    "\n",
    "hkr_wsl_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_matrix.csv\"\n",
    "hkr_pca_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_pca.csv\"\n",
    "\n",
    "spk_json = \"/home/gavinlouuu/coding/uom_explore/data_science/parameter.json\"\n",
    "hkr_wsl_json = \"/home/hk-wsl/code/uom_explore/data_science/parameter.json\"\n",
    "\n",
    "data_path = spk_pca_data\n",
    "param_path = hkr_wsl_json\n",
    "\n",
    "with open('parameter.json','r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# drop experiment_id column\n",
    "df.drop('experiment_id', axis=1, inplace=True)\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data for NaN\n",
    "# df.isnull().sum()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove channel_id from the list\n",
    "\n",
    "# Hyperparameters\n",
    "# Extract parameters from the JSON object\n",
    "hidden_size = params['hidden_size']\n",
    "ground_truth = params['ground_truth']\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "learning_rate = params['learning_rate']\n",
    "momentum_value = params['momentum_value']\n",
    "dropout_rate = params['dropout']\n",
    "\n",
    "features_raw = params['PCA_features']\n",
    "# # Convert feature list to strings\n",
    "features = list(map(str, features_raw))\n",
    "\n",
    "# # Get header list and remove 'channel_id'\n",
    "# header = df.columns.values.tolist()\n",
    "# header.remove('channel_id')\n",
    "# # Select features and remove ground_truth from feature list\n",
    "# features = [col for col in header if col != ground_truth]\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(ground_truth, axis=1)\n",
    "# select features to be used\n",
    "X = X[features]\n",
    "\n",
    "input_size = len(X.columns)  # removing the ground truth from the number of columns counted\n",
    "num_classes = df[ground_truth].nunique()\n",
    "\n",
    "y = df[ground_truth]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # This makes 60%, 20%, 20%\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(0,255)) # \n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_scaled, y_train)\n",
    "val_dataset = TensorDataset(X_val_scaled, y_val)\n",
    "test_dataset = TensorDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.6084, Validation Accuracy: 0.3828\n",
      "Epoch 2, Validation Loss: 1.6094, Validation Accuracy: 0.3203\n",
      "Epoch 3, Validation Loss: 1.6061, Validation Accuracy: 0.3438\n",
      "Epoch 4, Validation Loss: 1.6006, Validation Accuracy: 0.3672\n",
      "Epoch 5, Validation Loss: 1.5901, Validation Accuracy: 0.4688\n",
      "Epoch 6, Validation Loss: 1.5775, Validation Accuracy: 0.4453\n",
      "Epoch 7, Validation Loss: 1.5661, Validation Accuracy: 0.5469\n",
      "Epoch 8, Validation Loss: 1.5527, Validation Accuracy: 0.6328\n",
      "Epoch 9, Validation Loss: 1.5388, Validation Accuracy: 0.6328\n",
      "Epoch 10, Validation Loss: 1.5274, Validation Accuracy: 0.6406\n",
      "Epoch 11, Validation Loss: 1.5093, Validation Accuracy: 0.6484\n",
      "Epoch 12, Validation Loss: 1.4973, Validation Accuracy: 0.6484\n",
      "Epoch 13, Validation Loss: 1.4825, Validation Accuracy: 0.6484\n",
      "Epoch 14, Validation Loss: 1.4717, Validation Accuracy: 0.6484\n",
      "Epoch 15, Validation Loss: 1.4570, Validation Accuracy: 0.6562\n",
      "Epoch 16, Validation Loss: 1.4474, Validation Accuracy: 0.6562\n",
      "Epoch 17, Validation Loss: 1.4334, Validation Accuracy: 0.6562\n",
      "Epoch 18, Validation Loss: 1.4218, Validation Accuracy: 0.6562\n",
      "Epoch 19, Validation Loss: 1.4195, Validation Accuracy: 0.6562\n",
      "Epoch 20, Validation Loss: 1.4151, Validation Accuracy: 0.6641\n",
      "Epoch 21, Validation Loss: 1.4048, Validation Accuracy: 0.6875\n",
      "Epoch 22, Validation Loss: 1.3942, Validation Accuracy: 0.6875\n",
      "Epoch 23, Validation Loss: 1.3874, Validation Accuracy: 0.6875\n",
      "Epoch 24, Validation Loss: 1.3701, Validation Accuracy: 0.7188\n",
      "Epoch 25, Validation Loss: 1.3612, Validation Accuracy: 0.7422\n",
      "Epoch 26, Validation Loss: 1.3498, Validation Accuracy: 0.7266\n",
      "Epoch 27, Validation Loss: 1.3494, Validation Accuracy: 0.7422\n",
      "Epoch 28, Validation Loss: 1.3368, Validation Accuracy: 0.7500\n",
      "Epoch 29, Validation Loss: 1.3306, Validation Accuracy: 0.7500\n",
      "Epoch 30, Validation Loss: 1.3336, Validation Accuracy: 0.7500\n",
      "Epoch 31, Validation Loss: 1.3302, Validation Accuracy: 0.7500\n",
      "Epoch 32, Validation Loss: 1.3223, Validation Accuracy: 0.7578\n",
      "Epoch 33, Validation Loss: 1.3232, Validation Accuracy: 0.7656\n",
      "Epoch 34, Validation Loss: 1.3166, Validation Accuracy: 0.7656\n",
      "Epoch 35, Validation Loss: 1.3048, Validation Accuracy: 0.7578\n",
      "Epoch 36, Validation Loss: 1.3048, Validation Accuracy: 0.7656\n",
      "Epoch 37, Validation Loss: 1.2987, Validation Accuracy: 0.7578\n",
      "Epoch 38, Validation Loss: 1.2906, Validation Accuracy: 0.7656\n",
      "Epoch 39, Validation Loss: 1.2853, Validation Accuracy: 0.7734\n",
      "Epoch 40, Validation Loss: 1.2898, Validation Accuracy: 0.7578\n",
      "Epoch 41, Validation Loss: 1.2881, Validation Accuracy: 0.7656\n",
      "Epoch 42, Validation Loss: 1.2796, Validation Accuracy: 0.7500\n",
      "Epoch 43, Validation Loss: 1.2708, Validation Accuracy: 0.7656\n",
      "Epoch 44, Validation Loss: 1.2705, Validation Accuracy: 0.7500\n",
      "Epoch 45, Validation Loss: 1.2665, Validation Accuracy: 0.7734\n",
      "Epoch 46, Validation Loss: 1.2646, Validation Accuracy: 0.7500\n",
      "Epoch 47, Validation Loss: 1.2622, Validation Accuracy: 0.7578\n",
      "Epoch 48, Validation Loss: 1.2627, Validation Accuracy: 0.7500\n",
      "Epoch 49, Validation Loss: 1.2547, Validation Accuracy: 0.7422\n",
      "Epoch 50, Validation Loss: 1.2523, Validation Accuracy: 0.7578\n",
      "Epoch 51, Validation Loss: 1.2494, Validation Accuracy: 0.7578\n",
      "Epoch 52, Validation Loss: 1.2429, Validation Accuracy: 0.7734\n",
      "Epoch 53, Validation Loss: 1.2372, Validation Accuracy: 0.7812\n",
      "Epoch 54, Validation Loss: 1.2401, Validation Accuracy: 0.7812\n",
      "Epoch 55, Validation Loss: 1.2324, Validation Accuracy: 0.7812\n",
      "Epoch 56, Validation Loss: 1.2360, Validation Accuracy: 0.7969\n",
      "Epoch 57, Validation Loss: 1.2383, Validation Accuracy: 0.7578\n",
      "Epoch 58, Validation Loss: 1.2258, Validation Accuracy: 0.7812\n",
      "Epoch 59, Validation Loss: 1.2225, Validation Accuracy: 0.7656\n",
      "Epoch 60, Validation Loss: 1.2217, Validation Accuracy: 0.7734\n",
      "Epoch 61, Validation Loss: 1.2205, Validation Accuracy: 0.7969\n",
      "Epoch 62, Validation Loss: 1.2178, Validation Accuracy: 0.8125\n",
      "Epoch 63, Validation Loss: 1.2103, Validation Accuracy: 0.7812\n",
      "Epoch 64, Validation Loss: 1.2155, Validation Accuracy: 0.7969\n",
      "Epoch 65, Validation Loss: 1.2206, Validation Accuracy: 0.8047\n",
      "Epoch 66, Validation Loss: 1.2195, Validation Accuracy: 0.7812\n",
      "Epoch 67, Validation Loss: 1.2212, Validation Accuracy: 0.7734\n",
      "Epoch 68, Validation Loss: 1.2189, Validation Accuracy: 0.7656\n",
      "Epoch 69, Validation Loss: 1.2098, Validation Accuracy: 0.7812\n",
      "Epoch 70, Validation Loss: 1.2077, Validation Accuracy: 0.7812\n",
      "Epoch 71, Validation Loss: 1.2148, Validation Accuracy: 0.7891\n",
      "Epoch 72, Validation Loss: 1.2048, Validation Accuracy: 0.7891\n",
      "Epoch 73, Validation Loss: 1.2069, Validation Accuracy: 0.7969\n",
      "Epoch 74, Validation Loss: 1.1996, Validation Accuracy: 0.7891\n",
      "Epoch 75, Validation Loss: 1.1961, Validation Accuracy: 0.7891\n",
      "Epoch 76, Validation Loss: 1.1965, Validation Accuracy: 0.7969\n",
      "Epoch 77, Validation Loss: 1.1941, Validation Accuracy: 0.7969\n",
      "Epoch 78, Validation Loss: 1.1939, Validation Accuracy: 0.7969\n",
      "Epoch 79, Validation Loss: 1.1947, Validation Accuracy: 0.7969\n",
      "Epoch 80, Validation Loss: 1.1916, Validation Accuracy: 0.7969\n",
      "Epoch 81, Validation Loss: 1.1859, Validation Accuracy: 0.7891\n",
      "Epoch 82, Validation Loss: 1.1873, Validation Accuracy: 0.8047\n",
      "Epoch 83, Validation Loss: 1.1864, Validation Accuracy: 0.8047\n",
      "Epoch 84, Validation Loss: 1.1836, Validation Accuracy: 0.8047\n",
      "Epoch 85, Validation Loss: 1.1873, Validation Accuracy: 0.7969\n",
      "Epoch 86, Validation Loss: 1.1790, Validation Accuracy: 0.7969\n",
      "Epoch 87, Validation Loss: 1.1743, Validation Accuracy: 0.7891\n",
      "Epoch 88, Validation Loss: 1.1780, Validation Accuracy: 0.7969\n",
      "Epoch 89, Validation Loss: 1.1752, Validation Accuracy: 0.7969\n",
      "Epoch 90, Validation Loss: 1.1723, Validation Accuracy: 0.7891\n",
      "Epoch 91, Validation Loss: 1.1796, Validation Accuracy: 0.7891\n",
      "Epoch 92, Validation Loss: 1.1768, Validation Accuracy: 0.7969\n",
      "Epoch 93, Validation Loss: 1.1679, Validation Accuracy: 0.7891\n",
      "Epoch 94, Validation Loss: 1.1727, Validation Accuracy: 0.7969\n",
      "Epoch 95, Validation Loss: 1.1707, Validation Accuracy: 0.7891\n",
      "Epoch 96, Validation Loss: 1.1647, Validation Accuracy: 0.7812\n",
      "Epoch 97, Validation Loss: 1.1641, Validation Accuracy: 0.7891\n",
      "Epoch 98, Validation Loss: 1.1653, Validation Accuracy: 0.7891\n",
      "Epoch 99, Validation Loss: 1.1693, Validation Accuracy: 0.7891\n",
      "Epoch 100, Validation Loss: 1.1592, Validation Accuracy: 0.7891\n",
      "Epoch 101, Validation Loss: 1.1597, Validation Accuracy: 0.7891\n",
      "Epoch 102, Validation Loss: 1.1638, Validation Accuracy: 0.7891\n",
      "Epoch 103, Validation Loss: 1.1564, Validation Accuracy: 0.7891\n",
      "Epoch 104, Validation Loss: 1.1549, Validation Accuracy: 0.8125\n",
      "Epoch 105, Validation Loss: 1.1529, Validation Accuracy: 0.8125\n",
      "Epoch 106, Validation Loss: 1.1493, Validation Accuracy: 0.8125\n",
      "Epoch 107, Validation Loss: 1.1491, Validation Accuracy: 0.7891\n",
      "Epoch 108, Validation Loss: 1.1502, Validation Accuracy: 0.7891\n",
      "Epoch 109, Validation Loss: 1.1509, Validation Accuracy: 0.7969\n",
      "Epoch 110, Validation Loss: 1.1457, Validation Accuracy: 0.8047\n",
      "Epoch 111, Validation Loss: 1.1498, Validation Accuracy: 0.7891\n",
      "Epoch 112, Validation Loss: 1.1469, Validation Accuracy: 0.7891\n",
      "Epoch 113, Validation Loss: 1.1434, Validation Accuracy: 0.8125\n",
      "Epoch 114, Validation Loss: 1.1440, Validation Accuracy: 0.8125\n",
      "Epoch 115, Validation Loss: 1.1387, Validation Accuracy: 0.8125\n",
      "Epoch 116, Validation Loss: 1.1378, Validation Accuracy: 0.8281\n",
      "Epoch 117, Validation Loss: 1.1385, Validation Accuracy: 0.8203\n",
      "Epoch 118, Validation Loss: 1.1393, Validation Accuracy: 0.8203\n",
      "Epoch 119, Validation Loss: 1.1356, Validation Accuracy: 0.8203\n",
      "Epoch 120, Validation Loss: 1.1339, Validation Accuracy: 0.8203\n",
      "Epoch 121, Validation Loss: 1.1366, Validation Accuracy: 0.8203\n",
      "Epoch 122, Validation Loss: 1.1346, Validation Accuracy: 0.8281\n",
      "Epoch 123, Validation Loss: 1.1315, Validation Accuracy: 0.8281\n",
      "Epoch 124, Validation Loss: 1.1311, Validation Accuracy: 0.8281\n",
      "Epoch 125, Validation Loss: 1.1309, Validation Accuracy: 0.8281\n",
      "Epoch 126, Validation Loss: 1.1312, Validation Accuracy: 0.8203\n",
      "Epoch 127, Validation Loss: 1.1302, Validation Accuracy: 0.8281\n",
      "Epoch 128, Validation Loss: 1.1281, Validation Accuracy: 0.8281\n",
      "Epoch 129, Validation Loss: 1.1284, Validation Accuracy: 0.8281\n",
      "Epoch 130, Validation Loss: 1.1281, Validation Accuracy: 0.8281\n",
      "Epoch 131, Validation Loss: 1.1253, Validation Accuracy: 0.8281\n",
      "Epoch 132, Validation Loss: 1.1287, Validation Accuracy: 0.8359\n",
      "Epoch 133, Validation Loss: 1.1202, Validation Accuracy: 0.8359\n",
      "Epoch 134, Validation Loss: 1.1203, Validation Accuracy: 0.8281\n",
      "Epoch 135, Validation Loss: 1.1238, Validation Accuracy: 0.8359\n",
      "Epoch 136, Validation Loss: 1.1191, Validation Accuracy: 0.8359\n",
      "Epoch 137, Validation Loss: 1.1186, Validation Accuracy: 0.8281\n",
      "Epoch 138, Validation Loss: 1.1167, Validation Accuracy: 0.8359\n",
      "Epoch 139, Validation Loss: 1.1147, Validation Accuracy: 0.8281\n",
      "Epoch 140, Validation Loss: 1.1160, Validation Accuracy: 0.8359\n",
      "Epoch 141, Validation Loss: 1.1153, Validation Accuracy: 0.8359\n",
      "Epoch 142, Validation Loss: 1.1142, Validation Accuracy: 0.8359\n",
      "Epoch 143, Validation Loss: 1.1123, Validation Accuracy: 0.8281\n",
      "Epoch 144, Validation Loss: 1.1102, Validation Accuracy: 0.8281\n",
      "Epoch 145, Validation Loss: 1.1087, Validation Accuracy: 0.8281\n",
      "Epoch 146, Validation Loss: 1.1100, Validation Accuracy: 0.8281\n",
      "Epoch 147, Validation Loss: 1.1105, Validation Accuracy: 0.8438\n",
      "Epoch 148, Validation Loss: 1.1068, Validation Accuracy: 0.8359\n",
      "Epoch 149, Validation Loss: 1.1070, Validation Accuracy: 0.8359\n",
      "Epoch 150, Validation Loss: 1.1078, Validation Accuracy: 0.8359\n",
      "Epoch 151, Validation Loss: 1.1060, Validation Accuracy: 0.8438\n",
      "Epoch 152, Validation Loss: 1.1098, Validation Accuracy: 0.8516\n",
      "Epoch 153, Validation Loss: 1.1029, Validation Accuracy: 0.8359\n",
      "Epoch 154, Validation Loss: 1.1026, Validation Accuracy: 0.8438\n",
      "Epoch 155, Validation Loss: 1.1033, Validation Accuracy: 0.8516\n",
      "Epoch 156, Validation Loss: 1.1015, Validation Accuracy: 0.8594\n",
      "Epoch 157, Validation Loss: 1.0992, Validation Accuracy: 0.8516\n",
      "Epoch 158, Validation Loss: 1.0984, Validation Accuracy: 0.8516\n",
      "Epoch 159, Validation Loss: 1.1015, Validation Accuracy: 0.8438\n",
      "Epoch 160, Validation Loss: 1.0954, Validation Accuracy: 0.8438\n",
      "Epoch 161, Validation Loss: 1.0969, Validation Accuracy: 0.8672\n",
      "Epoch 162, Validation Loss: 1.0935, Validation Accuracy: 0.8672\n",
      "Epoch 163, Validation Loss: 1.0941, Validation Accuracy: 0.8438\n",
      "Epoch 164, Validation Loss: 1.0937, Validation Accuracy: 0.8672\n",
      "Epoch 165, Validation Loss: 1.0928, Validation Accuracy: 0.8594\n",
      "Epoch 166, Validation Loss: 1.0945, Validation Accuracy: 0.8672\n",
      "Epoch 167, Validation Loss: 1.0910, Validation Accuracy: 0.8438\n",
      "Epoch 168, Validation Loss: 1.0920, Validation Accuracy: 0.8750\n",
      "Epoch 169, Validation Loss: 1.0921, Validation Accuracy: 0.8750\n",
      "Epoch 170, Validation Loss: 1.0881, Validation Accuracy: 0.8750\n",
      "Epoch 171, Validation Loss: 1.0908, Validation Accuracy: 0.8672\n",
      "Epoch 172, Validation Loss: 1.0907, Validation Accuracy: 0.8672\n",
      "Epoch 173, Validation Loss: 1.0889, Validation Accuracy: 0.8594\n",
      "Epoch 174, Validation Loss: 1.0901, Validation Accuracy: 0.8594\n",
      "Epoch 175, Validation Loss: 1.0877, Validation Accuracy: 0.8594\n",
      "Epoch 176, Validation Loss: 1.0873, Validation Accuracy: 0.8906\n",
      "Epoch 177, Validation Loss: 1.0875, Validation Accuracy: 0.8906\n",
      "Epoch 178, Validation Loss: 1.0835, Validation Accuracy: 0.8906\n",
      "Epoch 179, Validation Loss: 1.0886, Validation Accuracy: 0.8906\n",
      "Epoch 180, Validation Loss: 1.0852, Validation Accuracy: 0.8984\n",
      "Epoch 181, Validation Loss: 1.0889, Validation Accuracy: 0.8906\n",
      "Epoch 182, Validation Loss: 1.0825, Validation Accuracy: 0.8906\n",
      "Epoch 183, Validation Loss: 1.0843, Validation Accuracy: 0.8828\n",
      "Epoch 184, Validation Loss: 1.0847, Validation Accuracy: 0.8984\n",
      "Epoch 185, Validation Loss: 1.0826, Validation Accuracy: 0.8906\n",
      "Epoch 186, Validation Loss: 1.0799, Validation Accuracy: 0.8984\n",
      "Epoch 187, Validation Loss: 1.0770, Validation Accuracy: 0.8906\n",
      "Epoch 188, Validation Loss: 1.0791, Validation Accuracy: 0.8984\n",
      "Epoch 189, Validation Loss: 1.0780, Validation Accuracy: 0.8906\n",
      "Epoch 190, Validation Loss: 1.0812, Validation Accuracy: 0.8984\n",
      "Epoch 191, Validation Loss: 1.0792, Validation Accuracy: 0.8984\n",
      "Epoch 192, Validation Loss: 1.0767, Validation Accuracy: 0.8984\n",
      "Epoch 193, Validation Loss: 1.0788, Validation Accuracy: 0.8984\n",
      "Epoch 194, Validation Loss: 1.0800, Validation Accuracy: 0.8984\n",
      "Epoch 195, Validation Loss: 1.0784, Validation Accuracy: 0.9062\n",
      "Epoch 196, Validation Loss: 1.0762, Validation Accuracy: 0.8984\n",
      "Epoch 197, Validation Loss: 1.0751, Validation Accuracy: 0.8984\n",
      "Epoch 198, Validation Loss: 1.0731, Validation Accuracy: 0.8984\n",
      "Epoch 199, Validation Loss: 1.0727, Validation Accuracy: 0.8984\n",
      "Epoch 200, Validation Loss: 1.0721, Validation Accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "# # Define the MLP model\n",
    "    \n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_prob):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Softmax activation for the output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "            x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.layers[-1](x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPClassifier(input_size, hidden_size, num_classes, dropout_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "# Function to predict the class of new data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        _, predicted_class = torch.max(output, dim=1)\n",
    "    return predicted_class\n",
    "\n",
    "# Function to compute the accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred, dim=1)  # Get the index of the max log-probability\n",
    "    correct = (predicted == y_true).float().sum()\n",
    "    return correct / y_true.shape[0]\n",
    "\n",
    "# Training and evaluation loop\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # scheduler.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss += criterion(y_val_pred, y_val).item()\n",
    "                val_accuracy += calculate_accuracy(y_val_pred, y_val)\n",
    "                _, predicted_classes = torch.max(y_val_pred, dim=1)\n",
    "                # print(f'Predicted: {predicted_classes}, Actual: {y_val}') # actual predicted values\n",
    "        \n",
    "        \n",
    "        # Average the loss and accuracy over all validation batches\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Assuming the rest of your setup (model initialization, data loaders, etc.) is already done\n",
    "# Now you would just call train_and_evaluate\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-29 18:22:12,927] A new study created in memory with name: no-name-1dda0dba-a664-4f1c-a574-6c95f08156a0\n",
      "C:\\Users\\gavin\\AppData\\Local\\Temp\\ipykernel_25040\\3152471334.py:46: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2024-07-29 18:22:17,071] Trial 0 finished with value: 0.90625 and parameters: {'num_layers': 3, 'hidden_size_0': 152, 'hidden_size_1': 67, 'hidden_size_2': 107, 'dropout_prob': 0.1498026405266693, 'learning_rate': 0.00023546533760684894}. Best is trial 0 with value: 0.90625.\n",
      "[I 2024-07-29 18:22:19,561] Trial 1 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 223, 'dropout_prob': 0.17811266304731516, 'learning_rate': 0.0014861909138276365}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:23,100] Trial 2 finished with value: 0.890625 and parameters: {'num_layers': 2, 'hidden_size_0': 163, 'hidden_size_1': 89, 'dropout_prob': 0.388463092382339, 'learning_rate': 0.0002922865955114585}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:25,524] Trial 3 finished with value: 0.875 and parameters: {'num_layers': 1, 'hidden_size_0': 212, 'dropout_prob': 0.11006164031813084, 'learning_rate': 0.00010855488064866465}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:27,872] Trial 4 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 106, 'dropout_prob': 0.20758463614645717, 'learning_rate': 0.008302686951910027}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:31,130] Trial 5 finished with value: 0.9140625 and parameters: {'num_layers': 2, 'hidden_size_0': 55, 'hidden_size_1': 186, 'dropout_prob': 0.2420935153854436, 'learning_rate': 0.0030750139311064954}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:34,376] Trial 6 finished with value: 0.8984375 and parameters: {'num_layers': 2, 'hidden_size_0': 62, 'hidden_size_1': 165, 'dropout_prob': 0.45351554971274044, 'learning_rate': 0.004679827907465009}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:37,626] Trial 7 finished with value: 0.90625 and parameters: {'num_layers': 2, 'hidden_size_0': 81, 'hidden_size_1': 240, 'dropout_prob': 0.12670142590267083, 'learning_rate': 0.0002017357073288838}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:40,703] Trial 8 finished with value: 0.8984375 and parameters: {'num_layers': 2, 'hidden_size_0': 66, 'hidden_size_1': 43, 'dropout_prob': 0.29197791954955227, 'learning_rate': 0.0012902567434435704}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:43,912] Trial 9 finished with value: 0.90625 and parameters: {'num_layers': 2, 'hidden_size_0': 49, 'hidden_size_1': 217, 'dropout_prob': 0.40727376703154894, 'learning_rate': 0.0001983101158085147}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:46,308] Trial 10 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 225, 'dropout_prob': 0.327549550684362, 'learning_rate': 0.0007796560172226522}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:48,686] Trial 11 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 242, 'dropout_prob': 0.32396444200008945, 'learning_rate': 0.0008256120871318741}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:51,061] Trial 12 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 205, 'dropout_prob': 0.3166994795044582, 'learning_rate': 0.0009890482383454718}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:53,473] Trial 13 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 247, 'dropout_prob': 0.2124861025064797, 'learning_rate': 0.0019812370653513227}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:22:57,892] Trial 14 finished with value: 0.9140625 and parameters: {'num_layers': 3, 'hidden_size_0': 191, 'hidden_size_1': 126, 'hidden_size_2': 245, 'dropout_prob': 0.36850845424376844, 'learning_rate': 0.0004898018847066413}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:00,237] Trial 15 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 120, 'dropout_prob': 0.2683214163501608, 'learning_rate': 0.0006013210735377907}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:02,574] Trial 16 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 176, 'dropout_prob': 0.17457292278904285, 'learning_rate': 0.0016813435261667264}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:04,969] Trial 17 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 181, 'dropout_prob': 0.17013130023620865, 'learning_rate': 0.002470490342500723}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:09,459] Trial 18 finished with value: 0.8984375 and parameters: {'num_layers': 3, 'hidden_size_0': 129, 'hidden_size_1': 121, 'hidden_size_2': 34, 'dropout_prob': 0.19138931687382704, 'learning_rate': 0.0016501689972874376}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:11,833] Trial 19 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 177, 'dropout_prob': 0.10070117774423962, 'learning_rate': 0.003481812941931787}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:15,458] Trial 20 finished with value: 0.921875 and parameters: {'num_layers': 2, 'hidden_size_0': 216, 'hidden_size_1': 255, 'dropout_prob': 0.23987787571496738, 'learning_rate': 0.00558035073813187}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:17,816] Trial 21 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 232, 'dropout_prob': 0.3481244713859615, 'learning_rate': 0.0005583079964558213}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:20,217] Trial 22 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 255, 'dropout_prob': 0.2720446479434453, 'learning_rate': 0.00135054436255455}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:22,587] Trial 23 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 225, 'dropout_prob': 0.15171300175666091, 'learning_rate': 0.0007122943520954332}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:25,012] Trial 24 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 196, 'dropout_prob': 0.48281253859336026, 'learning_rate': 0.000421213123677781}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:27,432] Trial 25 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 169, 'dropout_prob': 0.17726900680000976, 'learning_rate': 0.0011252698517107821}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:29,815] Trial 26 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 143, 'dropout_prob': 0.24353350987621009, 'learning_rate': 0.001923976265206819}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:32,188] Trial 27 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 142, 'dropout_prob': 0.22741153063084033, 'learning_rate': 0.002108167103539154}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:35,491] Trial 28 finished with value: 0.90625 and parameters: {'num_layers': 2, 'hidden_size_0': 100, 'hidden_size_1': 198, 'dropout_prob': 0.14888022690699954, 'learning_rate': 0.003936957867643844}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:39,753] Trial 29 finished with value: 0.9140625 and parameters: {'num_layers': 3, 'hidden_size_0': 150, 'hidden_size_1': 93, 'hidden_size_2': 255, 'dropout_prob': 0.14237098893463174, 'learning_rate': 0.007017456641698447}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:42,141] Trial 30 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 161, 'dropout_prob': 0.2641291521443778, 'learning_rate': 0.0016395314680333606}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:44,428] Trial 31 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 32, 'dropout_prob': 0.1905027858242669, 'learning_rate': 0.0026544231317635653}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:46,806] Trial 32 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 193, 'dropout_prob': 0.3011599158864703, 'learning_rate': 0.0003482483817771865}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:49,190] Trial 33 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 229, 'dropout_prob': 0.3410379126919493, 'learning_rate': 0.0008085650404351433}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:51,606] Trial 34 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 212, 'dropout_prob': 0.2053537101798236, 'learning_rate': 0.001509403350126077}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:53,990] Trial 35 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 126, 'dropout_prob': 0.404651369810311, 'learning_rate': 0.0009500204884240143}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:56,336] Trial 36 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 145, 'dropout_prob': 0.24567095928609578, 'learning_rate': 0.0020591589834344447}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:23:59,707] Trial 37 finished with value: 0.8828125 and parameters: {'num_layers': 2, 'hidden_size_0': 166, 'hidden_size_1': 149, 'dropout_prob': 0.126187158022051, 'learning_rate': 0.00013045534492330064}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:02,029] Trial 38 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 104, 'dropout_prob': 0.1688398650825355, 'learning_rate': 0.0012123495746584527}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:05,470] Trial 39 finished with value: 0.8984375 and parameters: {'num_layers': 2, 'hidden_size_0': 181, 'hidden_size_1': 217, 'dropout_prob': 0.22102407448331124, 'learning_rate': 0.003008572421154987}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:07,901] Trial 40 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 204, 'dropout_prob': 0.2905275188091117, 'learning_rate': 0.0006857829544608102}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:10,434] Trial 41 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 239, 'dropout_prob': 0.3261181036634485, 'learning_rate': 0.0008507903052940502}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:12,840] Trial 42 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 242, 'dropout_prob': 0.3731852965690374, 'learning_rate': 0.0010649298598864778}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:15,188] Trial 43 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 224, 'dropout_prob': 0.312995778639086, 'learning_rate': 0.0017422051226254635}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:17,566] Trial 44 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 250, 'dropout_prob': 0.3506546096000359, 'learning_rate': 0.0002976697071802294}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:20,750] Trial 45 finished with value: 0.890625 and parameters: {'num_layers': 2, 'hidden_size_0': 215, 'hidden_size_1': 46, 'dropout_prob': 0.26683544966447026, 'learning_rate': 0.00043907526960405087}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:23,100] Trial 46 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 203, 'dropout_prob': 0.24837706543981103, 'learning_rate': 0.00135792181510722}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:25,458] Trial 47 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 203, 'dropout_prob': 0.24750902213344755, 'learning_rate': 0.002262338541486519}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:27,810] Trial 48 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 198, 'dropout_prob': 0.24632229185640975, 'learning_rate': 0.0045697776055292445}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:30,195] Trial 49 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 157, 'dropout_prob': 0.20145820170833892, 'learning_rate': 0.002373479179436887}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:33,558] Trial 50 finished with value: 0.921875 and parameters: {'num_layers': 2, 'hidden_size_0': 182, 'hidden_size_1': 104, 'dropout_prob': 0.22936147799168233, 'learning_rate': 0.0030268351223288283}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:35,906] Trial 51 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 157, 'dropout_prob': 0.19735263245747003, 'learning_rate': 0.002501225373618167}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:38,261] Trial 52 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 171, 'dropout_prob': 0.1766090437516507, 'learning_rate': 0.0021258307955512885}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:40,579] Trial 53 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 136, 'dropout_prob': 0.2171919415092199, 'learning_rate': 0.0013655236459825452}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:42,940] Trial 54 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 188, 'dropout_prob': 0.2554925232469963, 'learning_rate': 0.0018347750083087722}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:45,285] Trial 55 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 204, 'dropout_prob': 0.28425122038980183, 'learning_rate': 0.003493467351294305}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:47,597] Trial 56 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 112, 'dropout_prob': 0.1268403899434013, 'learning_rate': 0.0024032509535558168}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:49,934] Trial 57 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 176, 'dropout_prob': 0.18862772265326183, 'learning_rate': 0.0014075492898675632}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:52,286] Trial 58 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 153, 'dropout_prob': 0.16233732535870063, 'learning_rate': 0.004270763277425048}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:54,635] Trial 59 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 187, 'dropout_prob': 0.20695570914531533, 'learning_rate': 0.0053311326324184975}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:56,947] Trial 60 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 92, 'dropout_prob': 0.23817921861133265, 'learning_rate': 0.001157781981679917}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:24:59,281] Trial 61 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 167, 'dropout_prob': 0.17956886236489794, 'learning_rate': 0.002210904175298158}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:01,641] Trial 62 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 173, 'dropout_prob': 0.20318094552785287, 'learning_rate': 0.0018254922234558733}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:04,026] Trial 63 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 141, 'dropout_prob': 0.1571482153953587, 'learning_rate': 0.002787027625648486}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:06,453] Trial 64 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 218, 'dropout_prob': 0.1823839979494628, 'learning_rate': 0.003479132088742241}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:08,808] Trial 65 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 208, 'dropout_prob': 0.1339723389789506, 'learning_rate': 0.001594507659815523}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:13,191] Trial 66 finished with value: 0.890625 and parameters: {'num_layers': 3, 'hidden_size_0': 200, 'hidden_size_1': 169, 'hidden_size_2': 158, 'dropout_prob': 0.23148172758511712, 'learning_rate': 0.0022191979589411912}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:15,534] Trial 67 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 161, 'dropout_prob': 0.2794453976860454, 'learning_rate': 0.001850334493830782}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:17,852] Trial 68 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 132, 'dropout_prob': 0.21535347290150672, 'learning_rate': 0.0014823835078782487}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:20,183] Trial 69 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 119, 'dropout_prob': 0.11075043837050341, 'learning_rate': 0.0009343285124577848}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:22,519] Trial 70 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 153, 'dropout_prob': 0.16581282979172418, 'learning_rate': 0.001982691428304828}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:24,896] Trial 71 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 209, 'dropout_prob': 0.13921347756328084, 'learning_rate': 0.0016184980485891144}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:27,264] Trial 72 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 193, 'dropout_prob': 0.13459054405121557, 'learning_rate': 0.0012598667222239747}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:29,640] Trial 73 finished with value: 0.875 and parameters: {'num_layers': 1, 'hidden_size_0': 222, 'dropout_prob': 0.25589624445804454, 'learning_rate': 0.0023166885301888333}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:31,987] Trial 74 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 172, 'dropout_prob': 0.17476632370258138, 'learning_rate': 0.0027148128155976476}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:34,389] Trial 75 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 208, 'dropout_prob': 0.11517987487809246, 'learning_rate': 0.0015342470753365127}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:36,814] Trial 76 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 182, 'dropout_prob': 0.15149506089709938, 'learning_rate': 0.0010807169805118195}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:39,260] Trial 77 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 239, 'dropout_prob': 0.19101765494855877, 'learning_rate': 0.0020586788709934865}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:41,618] Trial 78 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 160, 'dropout_prob': 0.25782321415992704, 'learning_rate': 0.003524207420272639}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:43,995] Trial 79 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 230, 'dropout_prob': 0.44397709471196334, 'learning_rate': 0.0012665582800013213}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:47,233] Trial 80 finished with value: 0.921875 and parameters: {'num_layers': 2, 'hidden_size_0': 146, 'hidden_size_1': 74, 'dropout_prob': 0.19934206922304848, 'learning_rate': 0.0015969617088223216}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:49,646] Trial 81 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 233, 'dropout_prob': 0.3055603360671815, 'learning_rate': 0.0006577159569621389}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:52,068] Trial 82 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 213, 'dropout_prob': 0.22701216571258864, 'learning_rate': 0.0008143616026509296}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:54,492] Trial 83 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 199, 'dropout_prob': 0.14720325969223033, 'learning_rate': 0.0009531517582270279}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:56,941] Trial 84 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 223, 'dropout_prob': 0.32386670368717824, 'learning_rate': 0.0005842298310234324}. Best is trial 1 with value: 0.9296875.\n",
      "[I 2024-07-29 18:25:59,367] Trial 85 finished with value: 0.9375 and parameters: {'num_layers': 1, 'hidden_size_0': 218, 'dropout_prob': 0.10160024659180605, 'learning_rate': 0.0017468635757977298}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:01,781] Trial 86 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 188, 'dropout_prob': 0.12002010755715792, 'learning_rate': 0.0018875427454159708}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:04,224] Trial 87 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 218, 'dropout_prob': 0.10837169135761204, 'learning_rate': 0.001712132915001477}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:06,662] Trial 88 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 205, 'dropout_prob': 0.1002843934663257, 'learning_rate': 0.0025326706945125363}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:09,099] Trial 89 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 178, 'dropout_prob': 0.16133920045951336, 'learning_rate': 0.0013891837656321683}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:11,531] Trial 90 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 138, 'dropout_prob': 0.13774209232399953, 'learning_rate': 0.0021087161040741383}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:13,965] Trial 91 finished with value: 0.90625 and parameters: {'num_layers': 1, 'hidden_size_0': 236, 'dropout_prob': 0.12793045947719384, 'learning_rate': 0.001137515190333753}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:16,388] Trial 92 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 220, 'dropout_prob': 0.336295339974571, 'learning_rate': 0.002975067580642357}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:18,803] Trial 93 finished with value: 0.921875 and parameters: {'num_layers': 1, 'hidden_size_0': 256, 'dropout_prob': 0.2948082451345532, 'learning_rate': 0.0016980233699469248}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:21,225] Trial 94 finished with value: 0.8984375 and parameters: {'num_layers': 1, 'hidden_size_0': 228, 'dropout_prob': 0.35957034331218674, 'learning_rate': 0.0014533285518841435}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:23,652] Trial 95 finished with value: 0.9296875 and parameters: {'num_layers': 1, 'hidden_size_0': 244, 'dropout_prob': 0.18598935184829687, 'learning_rate': 0.0007542047238769681}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:26,084] Trial 96 finished with value: 0.890625 and parameters: {'num_layers': 1, 'hidden_size_0': 247, 'dropout_prob': 0.21114634089317286, 'learning_rate': 0.002353229651196764}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:30,424] Trial 97 finished with value: 0.9140625 and parameters: {'num_layers': 3, 'hidden_size_0': 148, 'hidden_size_1': 141, 'hidden_size_2': 168, 'dropout_prob': 0.18346819595184954, 'learning_rate': 0.0010273272988312425}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:32,843] Trial 98 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 195, 'dropout_prob': 0.1960061757041747, 'learning_rate': 0.0012819582308826218}. Best is trial 85 with value: 0.9375.\n",
      "[I 2024-07-29 18:26:35,319] Trial 99 finished with value: 0.9140625 and parameters: {'num_layers': 1, 'hidden_size_0': 245, 'dropout_prob': 0.16906046602136873, 'learning_rate': 0.001955919509235421}. Best is trial 85 with value: 0.9375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_layers': 1, 'hidden_size_0': 218, 'dropout_prob': 0.10160024659180605, 'learning_rate': 0.0017468635757977298}\n",
      "Epoch 1, Validation Loss: 1.4028, Validation Accuracy: 0.7266\n",
      "Epoch 2, Validation Loss: 1.2402, Validation Accuracy: 0.7344\n",
      "Epoch 3, Validation Loss: 1.1865, Validation Accuracy: 0.7578\n",
      "Epoch 4, Validation Loss: 1.1643, Validation Accuracy: 0.8203\n",
      "Epoch 5, Validation Loss: 1.1506, Validation Accuracy: 0.7969\n",
      "Epoch 6, Validation Loss: 1.1334, Validation Accuracy: 0.8125\n",
      "Epoch 7, Validation Loss: 1.1207, Validation Accuracy: 0.8125\n",
      "Epoch 8, Validation Loss: 1.1126, Validation Accuracy: 0.8203\n",
      "Epoch 9, Validation Loss: 1.1091, Validation Accuracy: 0.8281\n",
      "Epoch 10, Validation Loss: 1.1043, Validation Accuracy: 0.8672\n",
      "Epoch 11, Validation Loss: 1.0985, Validation Accuracy: 0.8672\n",
      "Epoch 12, Validation Loss: 1.1023, Validation Accuracy: 0.8750\n",
      "Epoch 13, Validation Loss: 1.0882, Validation Accuracy: 0.8750\n",
      "Epoch 14, Validation Loss: 1.0817, Validation Accuracy: 0.8672\n",
      "Epoch 15, Validation Loss: 1.0810, Validation Accuracy: 0.8906\n",
      "Epoch 16, Validation Loss: 1.0864, Validation Accuracy: 0.8750\n",
      "Epoch 17, Validation Loss: 1.0783, Validation Accuracy: 0.8984\n",
      "Epoch 18, Validation Loss: 1.0761, Validation Accuracy: 0.8906\n",
      "Epoch 19, Validation Loss: 1.0729, Validation Accuracy: 0.8750\n",
      "Epoch 20, Validation Loss: 1.0660, Validation Accuracy: 0.8828\n",
      "Epoch 21, Validation Loss: 1.0648, Validation Accuracy: 0.8828\n",
      "Epoch 22, Validation Loss: 1.0700, Validation Accuracy: 0.8750\n",
      "Epoch 23, Validation Loss: 1.0635, Validation Accuracy: 0.8750\n",
      "Epoch 24, Validation Loss: 1.0594, Validation Accuracy: 0.8906\n",
      "Epoch 25, Validation Loss: 1.0640, Validation Accuracy: 0.8750\n",
      "Epoch 26, Validation Loss: 1.0627, Validation Accuracy: 0.8750\n",
      "Epoch 27, Validation Loss: 1.0533, Validation Accuracy: 0.8906\n",
      "Epoch 28, Validation Loss: 1.0546, Validation Accuracy: 0.8984\n",
      "Epoch 29, Validation Loss: 1.0548, Validation Accuracy: 0.8906\n",
      "Epoch 30, Validation Loss: 1.0534, Validation Accuracy: 0.8984\n",
      "Epoch 31, Validation Loss: 1.0617, Validation Accuracy: 0.8906\n",
      "Epoch 32, Validation Loss: 1.0466, Validation Accuracy: 0.8984\n",
      "Epoch 33, Validation Loss: 1.0453, Validation Accuracy: 0.8906\n",
      "Epoch 34, Validation Loss: 1.0525, Validation Accuracy: 0.8984\n",
      "Epoch 35, Validation Loss: 1.0453, Validation Accuracy: 0.8984\n",
      "Epoch 36, Validation Loss: 1.0407, Validation Accuracy: 0.8984\n",
      "Epoch 37, Validation Loss: 1.0502, Validation Accuracy: 0.8906\n",
      "Epoch 38, Validation Loss: 1.0514, Validation Accuracy: 0.8906\n",
      "Epoch 39, Validation Loss: 1.0383, Validation Accuracy: 0.8906\n",
      "Epoch 40, Validation Loss: 1.0398, Validation Accuracy: 0.9062\n",
      "Epoch 41, Validation Loss: 1.0405, Validation Accuracy: 0.9062\n",
      "Epoch 42, Validation Loss: 1.0378, Validation Accuracy: 0.9062\n",
      "Epoch 43, Validation Loss: 1.0431, Validation Accuracy: 0.8906\n",
      "Epoch 44, Validation Loss: 1.0509, Validation Accuracy: 0.8672\n",
      "Epoch 45, Validation Loss: 1.0454, Validation Accuracy: 0.8828\n",
      "Epoch 46, Validation Loss: 1.0359, Validation Accuracy: 0.8984\n",
      "Epoch 47, Validation Loss: 1.0284, Validation Accuracy: 0.8984\n",
      "Epoch 48, Validation Loss: 1.0333, Validation Accuracy: 0.8984\n",
      "Epoch 49, Validation Loss: 1.0317, Validation Accuracy: 0.8984\n",
      "Epoch 50, Validation Loss: 1.0264, Validation Accuracy: 0.8984\n",
      "Epoch 51, Validation Loss: 1.0280, Validation Accuracy: 0.8984\n",
      "Epoch 52, Validation Loss: 1.0293, Validation Accuracy: 0.9062\n",
      "Epoch 53, Validation Loss: 1.0286, Validation Accuracy: 0.9062\n",
      "Epoch 54, Validation Loss: 1.0263, Validation Accuracy: 0.8984\n",
      "Epoch 55, Validation Loss: 1.0347, Validation Accuracy: 0.8984\n",
      "Epoch 56, Validation Loss: 1.0246, Validation Accuracy: 0.8984\n",
      "Epoch 57, Validation Loss: 1.0278, Validation Accuracy: 0.9062\n",
      "Epoch 58, Validation Loss: 1.0390, Validation Accuracy: 0.8828\n",
      "Epoch 59, Validation Loss: 1.0527, Validation Accuracy: 0.8672\n",
      "Epoch 60, Validation Loss: 1.0310, Validation Accuracy: 0.8984\n",
      "Epoch 61, Validation Loss: 1.0257, Validation Accuracy: 0.9062\n",
      "Epoch 62, Validation Loss: 1.0214, Validation Accuracy: 0.9141\n",
      "Epoch 63, Validation Loss: 1.0232, Validation Accuracy: 0.9062\n",
      "Epoch 64, Validation Loss: 1.0216, Validation Accuracy: 0.8984\n",
      "Epoch 65, Validation Loss: 1.0256, Validation Accuracy: 0.8984\n",
      "Epoch 66, Validation Loss: 1.0296, Validation Accuracy: 0.8906\n",
      "Epoch 67, Validation Loss: 1.0269, Validation Accuracy: 0.8984\n",
      "Epoch 68, Validation Loss: 1.0289, Validation Accuracy: 0.8906\n",
      "Epoch 69, Validation Loss: 1.0190, Validation Accuracy: 0.9141\n",
      "Epoch 70, Validation Loss: 1.0168, Validation Accuracy: 0.9141\n",
      "Epoch 71, Validation Loss: 1.0173, Validation Accuracy: 0.9219\n",
      "Epoch 72, Validation Loss: 1.0241, Validation Accuracy: 0.8906\n",
      "Epoch 73, Validation Loss: 1.0161, Validation Accuracy: 0.9219\n",
      "Epoch 74, Validation Loss: 1.0256, Validation Accuracy: 0.8906\n",
      "Epoch 75, Validation Loss: 1.0229, Validation Accuracy: 0.8906\n",
      "Epoch 76, Validation Loss: 1.0162, Validation Accuracy: 0.9219\n",
      "Epoch 77, Validation Loss: 1.0235, Validation Accuracy: 0.8984\n",
      "Epoch 78, Validation Loss: 1.0189, Validation Accuracy: 0.8984\n",
      "Epoch 79, Validation Loss: 1.0204, Validation Accuracy: 0.8906\n",
      "Epoch 80, Validation Loss: 1.0204, Validation Accuracy: 0.9141\n",
      "Epoch 81, Validation Loss: 1.0288, Validation Accuracy: 0.8828\n",
      "Epoch 82, Validation Loss: 1.0183, Validation Accuracy: 0.9141\n",
      "Epoch 83, Validation Loss: 1.0131, Validation Accuracy: 0.8984\n",
      "Epoch 84, Validation Loss: 1.0216, Validation Accuracy: 0.8984\n",
      "Epoch 85, Validation Loss: 1.0343, Validation Accuracy: 0.8828\n",
      "Epoch 86, Validation Loss: 1.0252, Validation Accuracy: 0.8906\n",
      "Epoch 87, Validation Loss: 1.0232, Validation Accuracy: 0.8906\n",
      "Epoch 88, Validation Loss: 1.0134, Validation Accuracy: 0.9141\n",
      "Epoch 89, Validation Loss: 1.0197, Validation Accuracy: 0.8906\n",
      "Epoch 90, Validation Loss: 1.0152, Validation Accuracy: 0.9062\n",
      "Epoch 91, Validation Loss: 1.0129, Validation Accuracy: 0.9219\n",
      "Epoch 92, Validation Loss: 1.0133, Validation Accuracy: 0.9141\n",
      "Epoch 93, Validation Loss: 1.0104, Validation Accuracy: 0.9141\n",
      "Epoch 94, Validation Loss: 1.0216, Validation Accuracy: 0.9062\n",
      "Epoch 95, Validation Loss: 1.0227, Validation Accuracy: 0.8906\n",
      "Epoch 96, Validation Loss: 1.0186, Validation Accuracy: 0.8984\n",
      "Epoch 97, Validation Loss: 1.0119, Validation Accuracy: 0.9141\n",
      "Epoch 98, Validation Loss: 1.0118, Validation Accuracy: 0.9062\n",
      "Epoch 99, Validation Loss: 1.0275, Validation Accuracy: 0.8906\n",
      "Epoch 100, Validation Loss: 1.0079, Validation Accuracy: 0.9141\n",
      "Epoch 101, Validation Loss: 1.0189, Validation Accuracy: 0.8984\n",
      "Epoch 102, Validation Loss: 1.0119, Validation Accuracy: 0.9062\n",
      "Epoch 103, Validation Loss: 1.0041, Validation Accuracy: 0.9375\n",
      "Epoch 104, Validation Loss: 1.0037, Validation Accuracy: 0.9297\n",
      "Epoch 105, Validation Loss: 1.0177, Validation Accuracy: 0.8906\n",
      "Epoch 106, Validation Loss: 1.0167, Validation Accuracy: 0.8984\n",
      "Epoch 107, Validation Loss: 1.0139, Validation Accuracy: 0.8906\n",
      "Epoch 108, Validation Loss: 1.0278, Validation Accuracy: 0.8984\n",
      "Epoch 109, Validation Loss: 1.0236, Validation Accuracy: 0.8984\n",
      "Epoch 110, Validation Loss: 1.0143, Validation Accuracy: 0.9062\n",
      "Epoch 111, Validation Loss: 1.0071, Validation Accuracy: 0.9297\n",
      "Epoch 112, Validation Loss: 1.0130, Validation Accuracy: 0.9062\n",
      "Epoch 113, Validation Loss: 1.0072, Validation Accuracy: 0.9141\n",
      "Epoch 114, Validation Loss: 1.0124, Validation Accuracy: 0.8984\n",
      "Epoch 115, Validation Loss: 1.0083, Validation Accuracy: 0.9141\n",
      "Epoch 116, Validation Loss: 1.0081, Validation Accuracy: 0.9141\n",
      "Epoch 117, Validation Loss: 1.0128, Validation Accuracy: 0.8984\n",
      "Epoch 118, Validation Loss: 1.0195, Validation Accuracy: 0.8906\n",
      "Epoch 119, Validation Loss: 1.0127, Validation Accuracy: 0.8984\n",
      "Epoch 120, Validation Loss: 1.0107, Validation Accuracy: 0.8984\n",
      "Epoch 121, Validation Loss: 1.0187, Validation Accuracy: 0.8984\n",
      "Epoch 122, Validation Loss: 1.0277, Validation Accuracy: 0.8984\n",
      "Epoch 123, Validation Loss: 1.0196, Validation Accuracy: 0.8906\n",
      "Epoch 124, Validation Loss: 1.0060, Validation Accuracy: 0.9297\n",
      "Epoch 125, Validation Loss: 1.0066, Validation Accuracy: 0.9141\n",
      "Epoch 126, Validation Loss: 1.0143, Validation Accuracy: 0.8984\n",
      "Epoch 127, Validation Loss: 1.0119, Validation Accuracy: 0.8984\n",
      "Epoch 128, Validation Loss: 1.0089, Validation Accuracy: 0.9141\n",
      "Epoch 129, Validation Loss: 1.0060, Validation Accuracy: 0.9219\n",
      "Epoch 130, Validation Loss: 1.0010, Validation Accuracy: 0.9297\n",
      "Epoch 131, Validation Loss: 1.0154, Validation Accuracy: 0.8906\n",
      "Epoch 132, Validation Loss: 1.0065, Validation Accuracy: 0.8984\n",
      "Epoch 133, Validation Loss: 1.0048, Validation Accuracy: 0.9062\n",
      "Epoch 134, Validation Loss: 1.0213, Validation Accuracy: 0.8984\n",
      "Epoch 135, Validation Loss: 1.0069, Validation Accuracy: 0.9141\n",
      "Epoch 136, Validation Loss: 1.0032, Validation Accuracy: 0.9297\n",
      "Epoch 137, Validation Loss: 1.0060, Validation Accuracy: 0.9141\n",
      "Epoch 138, Validation Loss: 1.0059, Validation Accuracy: 0.9062\n",
      "Epoch 139, Validation Loss: 1.0055, Validation Accuracy: 0.9062\n",
      "Epoch 140, Validation Loss: 1.0014, Validation Accuracy: 0.9141\n",
      "Epoch 141, Validation Loss: 1.0062, Validation Accuracy: 0.9219\n",
      "Epoch 142, Validation Loss: 1.0080, Validation Accuracy: 0.9062\n",
      "Epoch 143, Validation Loss: 1.0044, Validation Accuracy: 0.9141\n",
      "Epoch 144, Validation Loss: 1.0048, Validation Accuracy: 0.8984\n",
      "Epoch 145, Validation Loss: 1.0006, Validation Accuracy: 0.9297\n",
      "Epoch 146, Validation Loss: 1.0064, Validation Accuracy: 0.9062\n",
      "Epoch 147, Validation Loss: 1.0111, Validation Accuracy: 0.8906\n",
      "Epoch 148, Validation Loss: 1.0133, Validation Accuracy: 0.8906\n",
      "Epoch 149, Validation Loss: 1.0137, Validation Accuracy: 0.8984\n",
      "Epoch 150, Validation Loss: 1.0064, Validation Accuracy: 0.9062\n",
      "Epoch 151, Validation Loss: 1.0073, Validation Accuracy: 0.9219\n",
      "Epoch 152, Validation Loss: 1.0169, Validation Accuracy: 0.8906\n",
      "Epoch 153, Validation Loss: 1.0068, Validation Accuracy: 0.8984\n",
      "Epoch 154, Validation Loss: 1.0028, Validation Accuracy: 0.9141\n",
      "Epoch 155, Validation Loss: 0.9993, Validation Accuracy: 0.9297\n",
      "Epoch 156, Validation Loss: 0.9994, Validation Accuracy: 0.9297\n",
      "Epoch 157, Validation Loss: 1.0070, Validation Accuracy: 0.9062\n",
      "Epoch 158, Validation Loss: 1.0002, Validation Accuracy: 0.9375\n",
      "Epoch 159, Validation Loss: 1.0154, Validation Accuracy: 0.8906\n",
      "Epoch 160, Validation Loss: 1.0142, Validation Accuracy: 0.8906\n",
      "Epoch 161, Validation Loss: 1.0016, Validation Accuracy: 0.9219\n",
      "Epoch 162, Validation Loss: 1.0006, Validation Accuracy: 0.9141\n",
      "Epoch 163, Validation Loss: 1.0029, Validation Accuracy: 0.9062\n",
      "Epoch 164, Validation Loss: 1.0129, Validation Accuracy: 0.8984\n",
      "Epoch 165, Validation Loss: 1.0161, Validation Accuracy: 0.8906\n",
      "Epoch 166, Validation Loss: 0.9984, Validation Accuracy: 0.9297\n",
      "Epoch 167, Validation Loss: 0.9990, Validation Accuracy: 0.9297\n",
      "Epoch 168, Validation Loss: 1.0028, Validation Accuracy: 0.9141\n",
      "Epoch 169, Validation Loss: 1.0067, Validation Accuracy: 0.9062\n",
      "Epoch 170, Validation Loss: 0.9999, Validation Accuracy: 0.9219\n",
      "Epoch 171, Validation Loss: 1.0001, Validation Accuracy: 0.9219\n",
      "Epoch 172, Validation Loss: 1.0007, Validation Accuracy: 0.9297\n",
      "Epoch 173, Validation Loss: 1.0105, Validation Accuracy: 0.9062\n",
      "Epoch 174, Validation Loss: 1.0261, Validation Accuracy: 0.8984\n",
      "Epoch 175, Validation Loss: 1.0023, Validation Accuracy: 0.9141\n",
      "Epoch 176, Validation Loss: 0.9985, Validation Accuracy: 0.9219\n",
      "Epoch 177, Validation Loss: 1.0004, Validation Accuracy: 0.9141\n",
      "Epoch 178, Validation Loss: 0.9993, Validation Accuracy: 0.9219\n",
      "Epoch 179, Validation Loss: 1.0028, Validation Accuracy: 0.9062\n",
      "Epoch 180, Validation Loss: 1.0191, Validation Accuracy: 0.8906\n",
      "Epoch 181, Validation Loss: 1.0197, Validation Accuracy: 0.8906\n",
      "Epoch 182, Validation Loss: 0.9943, Validation Accuracy: 0.9297\n",
      "Epoch 183, Validation Loss: 1.0009, Validation Accuracy: 0.9062\n",
      "Epoch 184, Validation Loss: 0.9947, Validation Accuracy: 0.9297\n",
      "Epoch 185, Validation Loss: 1.0013, Validation Accuracy: 0.9062\n",
      "Epoch 186, Validation Loss: 1.0323, Validation Accuracy: 0.8906\n",
      "Epoch 187, Validation Loss: 0.9999, Validation Accuracy: 0.9219\n",
      "Epoch 188, Validation Loss: 1.0057, Validation Accuracy: 0.8984\n",
      "Epoch 189, Validation Loss: 0.9955, Validation Accuracy: 0.9219\n",
      "Epoch 190, Validation Loss: 0.9984, Validation Accuracy: 0.9219\n",
      "Epoch 191, Validation Loss: 0.9977, Validation Accuracy: 0.9141\n",
      "Epoch 192, Validation Loss: 1.0024, Validation Accuracy: 0.9062\n",
      "Epoch 193, Validation Loss: 1.0135, Validation Accuracy: 0.8984\n",
      "Epoch 194, Validation Loss: 1.0059, Validation Accuracy: 0.8906\n",
      "Epoch 195, Validation Loss: 1.0020, Validation Accuracy: 0.9141\n",
      "Epoch 196, Validation Loss: 0.9975, Validation Accuracy: 0.9219\n",
      "Epoch 197, Validation Loss: 0.9998, Validation Accuracy: 0.9141\n",
      "Epoch 198, Validation Loss: 0.9976, Validation Accuracy: 0.9219\n",
      "Epoch 199, Validation Loss: 1.0096, Validation Accuracy: 0.8984\n",
      "Epoch 200, Validation Loss: 1.0154, Validation Accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_prob):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i + 1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Softmax activation for the output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "            x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.layers[-1](x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    hidden_sizes = [trial.suggest_int(f'hidden_size_{i}', 32, 256) for i in range(trial.suggest_int('num_layers', 1, 3))]\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLPClassifier(input_size, hidden_sizes, num_classes, dropout_prob)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training and evaluation loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss += criterion(y_val_pred, y_val).item()\n",
    "                val_accuracy += calculate_accuracy(y_val_pred, y_val)\n",
    "        \n",
    "        # Average the loss and accuracy over all validation batches\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= len(val_loader)\n",
    "    \n",
    "    return val_accuracy\n",
    "\n",
    "# Run the Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "hidden_sizes = [best_params[f'hidden_size_{i}'] for i in range(best_params['num_layers'])]\n",
    "dropout_prob = best_params['dropout_prob']\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "model = MLPClassifier(input_size, hidden_sizes, num_classes, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "trained_model = train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# in_channels = params['in_channels']\n",
    "# out_channels = params['out_channels']\n",
    "# kernel_sizes = params['kernel_sizes']\n",
    "\n",
    "# class CNN1DClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_sizes, num_classes, dropout_rate=0.5):\n",
    "#         super(CNN1DClassifier, self).__init__()\n",
    "        \n",
    "#         assert len(out_channels) == len(kernel_sizes), \"The length of out_channels and kernel_sizes must be the same\"\n",
    "        \n",
    "#         self.convs = nn.ModuleList()\n",
    "#         self.bns = nn.ModuleList()  # Adding batch normalization if needed\n",
    "        \n",
    "#         current_in_channels = in_channels\n",
    "        \n",
    "#         for out_channel, kernel_size in zip(out_channels, kernel_sizes):\n",
    "#             self.convs.append(nn.Conv1d(current_in_channels, out_channel, kernel_size=kernel_size, stride=1, padding=kernel_size // 2))\n",
    "#             self.bns.append(nn.BatchNorm1d(out_channel))  # Optional: Add batch normalization\n",
    "#             current_in_channels = out_channel\n",
    "        \n",
    "#         # Calculate the size after all convolutional and pooling layers\n",
    "#         conv_output_size = input_size\n",
    "#         for kernel_size in kernel_sizes:\n",
    "#             conv_output_size = (conv_output_size + 2 * (kernel_size // 2) - (kernel_size - 1) - 1) // 1 + 1\n",
    "#             conv_output_size = conv_output_size // 2  # After pooling\n",
    "        \n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(out_channels[-1] * conv_output_size, 128)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'Input shape: {x.shape}')\n",
    "#         for conv, bn in zip(self.convs, self.bns):\n",
    "#             x = self.pool(torch.relu(bn(conv(x))))\n",
    "#             # print(f'After conv and pool: {x.shape}')\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         # print(f'After flatten: {x.shape}')\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # print(f'After fc1: {x.shape}')\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(f'After fc2: {x.shape}')\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# # Train the 1D CNN model\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, device='cpu'):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "#         print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "#         print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_corrects = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "#                 val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         val_loss = val_loss / len(val_loader.dataset)\n",
    "#         val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "#         print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = CNN1DClassifier(in_channels, out_channels, kernel_sizes, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# test_corrects = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         test_loss += loss.item() * inputs.size(0)\n",
    "#         test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# test_loss = test_loss / len(test_loader.dataset)\n",
    "# test_acc = test_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
