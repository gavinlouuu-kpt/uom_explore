{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\code\\uom_explore\\.env\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.297113</td>\n",
       "      <td>-36.028848</td>\n",
       "      <td>-0.609223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.935828</td>\n",
       "      <td>-33.246099</td>\n",
       "      <td>-1.113492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.812983</td>\n",
       "      <td>-29.732352</td>\n",
       "      <td>-1.708627</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.757883</td>\n",
       "      <td>-26.591866</td>\n",
       "      <td>-1.720893</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.882516</td>\n",
       "      <td>-22.280264</td>\n",
       "      <td>0.086122</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PC1        PC2       PC3  channel_id\n",
       "0  -9.297113 -36.028848 -0.609223           0\n",
       "1  -7.935828 -33.246099 -1.113492           1\n",
       "2  -6.812983 -29.732352 -1.708627           2\n",
       "3  -5.757883 -26.591866 -1.720893           3\n",
       "4 -17.882516 -22.280264  0.086122           4"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spk_data = \"D:\\\\code\\\\uom_explore\\\\model_input\\\\3_features.csv\"\n",
    "spk_pca_data = \"D:\\\\code\\\\uom_explore\\\\model_input\\\\pca_df.csv\"\n",
    "\n",
    "hkr_wsl_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_matrix.csv\"\n",
    "hkr_pca_data = \"/home/hk-wsl/code/uom_explore/model_input/feature_pca.csv\"\n",
    "\n",
    "spk_json = \"/home/gavinlouuu/coding/uom_explore/data_science/parameter.json\"\n",
    "hkr_wsl_json = \"/home/hk-wsl/code/uom_explore/data_science/parameter.json\"\n",
    "\n",
    "data_path = spk_pca_data\n",
    "param_path = hkr_wsl_json\n",
    "\n",
    "with open('parameter.json','r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# drop experiment_id column\n",
    "df.drop('experiment_id', axis=1, inplace=True)\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data for NaN\n",
    "# df.isnull().sum()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove channel_id from the list\n",
    "\n",
    "# Hyperparameters\n",
    "# Extract parameters from the JSON object\n",
    "hidden_size = params['hidden_size']\n",
    "ground_truth = params['ground_truth']\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "learning_rate = params['learning_rate']\n",
    "momentum_value = params['momentum_value']\n",
    "dropout_rate = params['dropout']\n",
    "\n",
    "features_raw = params['PCA_features']\n",
    "# # Convert feature list to strings\n",
    "features = list(map(str, features_raw))\n",
    "\n",
    "# # Get header list and remove 'channel_id'\n",
    "# header = df.columns.values.tolist()\n",
    "# header.remove('channel_id')\n",
    "# # Select features and remove ground_truth from feature list\n",
    "# features = [col for col in header if col != ground_truth]\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(ground_truth, axis=1)\n",
    "# select features to be used\n",
    "X = X[features]\n",
    "\n",
    "input_size = len(X.columns)  # removing the ground truth from the number of columns counted\n",
    "num_classes = df[ground_truth].nunique()\n",
    "\n",
    "y = df[ground_truth]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # This makes 60%, 20%, 20%\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(0,255)) # \n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)#.unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_scaled, y_train)\n",
    "val_dataset = TensorDataset(X_val_scaled, y_val)\n",
    "test_dataset = TensorDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.6084, Validation Accuracy: 0.3828\n",
      "Epoch 2, Validation Loss: 1.6094, Validation Accuracy: 0.3203\n",
      "Epoch 3, Validation Loss: 1.6061, Validation Accuracy: 0.3438\n",
      "Epoch 4, Validation Loss: 1.6006, Validation Accuracy: 0.3672\n",
      "Epoch 5, Validation Loss: 1.5901, Validation Accuracy: 0.4688\n",
      "Epoch 6, Validation Loss: 1.5775, Validation Accuracy: 0.4453\n",
      "Epoch 7, Validation Loss: 1.5661, Validation Accuracy: 0.5469\n",
      "Epoch 8, Validation Loss: 1.5527, Validation Accuracy: 0.6328\n",
      "Epoch 9, Validation Loss: 1.5388, Validation Accuracy: 0.6328\n",
      "Epoch 10, Validation Loss: 1.5274, Validation Accuracy: 0.6406\n",
      "Epoch 11, Validation Loss: 1.5093, Validation Accuracy: 0.6484\n",
      "Epoch 12, Validation Loss: 1.4973, Validation Accuracy: 0.6484\n",
      "Epoch 13, Validation Loss: 1.4825, Validation Accuracy: 0.6484\n",
      "Epoch 14, Validation Loss: 1.4717, Validation Accuracy: 0.6484\n",
      "Epoch 15, Validation Loss: 1.4570, Validation Accuracy: 0.6562\n",
      "Epoch 16, Validation Loss: 1.4474, Validation Accuracy: 0.6562\n",
      "Epoch 17, Validation Loss: 1.4334, Validation Accuracy: 0.6562\n",
      "Epoch 18, Validation Loss: 1.4218, Validation Accuracy: 0.6562\n",
      "Epoch 19, Validation Loss: 1.4195, Validation Accuracy: 0.6562\n",
      "Epoch 20, Validation Loss: 1.4151, Validation Accuracy: 0.6641\n",
      "Epoch 21, Validation Loss: 1.4048, Validation Accuracy: 0.6875\n",
      "Epoch 22, Validation Loss: 1.3942, Validation Accuracy: 0.6875\n",
      "Epoch 23, Validation Loss: 1.3874, Validation Accuracy: 0.6875\n",
      "Epoch 24, Validation Loss: 1.3701, Validation Accuracy: 0.7188\n",
      "Epoch 25, Validation Loss: 1.3612, Validation Accuracy: 0.7422\n",
      "Epoch 26, Validation Loss: 1.3498, Validation Accuracy: 0.7266\n",
      "Epoch 27, Validation Loss: 1.3494, Validation Accuracy: 0.7422\n",
      "Epoch 28, Validation Loss: 1.3368, Validation Accuracy: 0.7500\n",
      "Epoch 29, Validation Loss: 1.3306, Validation Accuracy: 0.7500\n",
      "Epoch 30, Validation Loss: 1.3336, Validation Accuracy: 0.7500\n",
      "Epoch 31, Validation Loss: 1.3302, Validation Accuracy: 0.7500\n",
      "Epoch 32, Validation Loss: 1.3223, Validation Accuracy: 0.7578\n",
      "Epoch 33, Validation Loss: 1.3232, Validation Accuracy: 0.7656\n",
      "Epoch 34, Validation Loss: 1.3166, Validation Accuracy: 0.7656\n",
      "Epoch 35, Validation Loss: 1.3048, Validation Accuracy: 0.7578\n",
      "Epoch 36, Validation Loss: 1.3048, Validation Accuracy: 0.7656\n",
      "Epoch 37, Validation Loss: 1.2987, Validation Accuracy: 0.7578\n",
      "Epoch 38, Validation Loss: 1.2906, Validation Accuracy: 0.7656\n",
      "Epoch 39, Validation Loss: 1.2853, Validation Accuracy: 0.7734\n",
      "Epoch 40, Validation Loss: 1.2898, Validation Accuracy: 0.7578\n",
      "Epoch 41, Validation Loss: 1.2881, Validation Accuracy: 0.7656\n",
      "Epoch 42, Validation Loss: 1.2796, Validation Accuracy: 0.7500\n",
      "Epoch 43, Validation Loss: 1.2708, Validation Accuracy: 0.7656\n",
      "Epoch 44, Validation Loss: 1.2705, Validation Accuracy: 0.7500\n",
      "Epoch 45, Validation Loss: 1.2665, Validation Accuracy: 0.7734\n",
      "Epoch 46, Validation Loss: 1.2646, Validation Accuracy: 0.7500\n",
      "Epoch 47, Validation Loss: 1.2622, Validation Accuracy: 0.7578\n",
      "Epoch 48, Validation Loss: 1.2627, Validation Accuracy: 0.7500\n",
      "Epoch 49, Validation Loss: 1.2547, Validation Accuracy: 0.7422\n",
      "Epoch 50, Validation Loss: 1.2523, Validation Accuracy: 0.7578\n",
      "Epoch 51, Validation Loss: 1.2494, Validation Accuracy: 0.7578\n",
      "Epoch 52, Validation Loss: 1.2429, Validation Accuracy: 0.7734\n",
      "Epoch 53, Validation Loss: 1.2372, Validation Accuracy: 0.7812\n",
      "Epoch 54, Validation Loss: 1.2401, Validation Accuracy: 0.7812\n",
      "Epoch 55, Validation Loss: 1.2324, Validation Accuracy: 0.7812\n",
      "Epoch 56, Validation Loss: 1.2360, Validation Accuracy: 0.7969\n",
      "Epoch 57, Validation Loss: 1.2383, Validation Accuracy: 0.7578\n",
      "Epoch 58, Validation Loss: 1.2258, Validation Accuracy: 0.7812\n",
      "Epoch 59, Validation Loss: 1.2225, Validation Accuracy: 0.7656\n",
      "Epoch 60, Validation Loss: 1.2217, Validation Accuracy: 0.7734\n",
      "Epoch 61, Validation Loss: 1.2205, Validation Accuracy: 0.7969\n",
      "Epoch 62, Validation Loss: 1.2178, Validation Accuracy: 0.8125\n",
      "Epoch 63, Validation Loss: 1.2103, Validation Accuracy: 0.7812\n",
      "Epoch 64, Validation Loss: 1.2155, Validation Accuracy: 0.7969\n",
      "Epoch 65, Validation Loss: 1.2206, Validation Accuracy: 0.8047\n",
      "Epoch 66, Validation Loss: 1.2195, Validation Accuracy: 0.7812\n",
      "Epoch 67, Validation Loss: 1.2212, Validation Accuracy: 0.7734\n",
      "Epoch 68, Validation Loss: 1.2189, Validation Accuracy: 0.7656\n",
      "Epoch 69, Validation Loss: 1.2098, Validation Accuracy: 0.7812\n",
      "Epoch 70, Validation Loss: 1.2077, Validation Accuracy: 0.7812\n",
      "Epoch 71, Validation Loss: 1.2148, Validation Accuracy: 0.7891\n",
      "Epoch 72, Validation Loss: 1.2048, Validation Accuracy: 0.7891\n",
      "Epoch 73, Validation Loss: 1.2069, Validation Accuracy: 0.7969\n",
      "Epoch 74, Validation Loss: 1.1996, Validation Accuracy: 0.7891\n",
      "Epoch 75, Validation Loss: 1.1961, Validation Accuracy: 0.7891\n",
      "Epoch 76, Validation Loss: 1.1965, Validation Accuracy: 0.7969\n",
      "Epoch 77, Validation Loss: 1.1941, Validation Accuracy: 0.7969\n",
      "Epoch 78, Validation Loss: 1.1939, Validation Accuracy: 0.7969\n",
      "Epoch 79, Validation Loss: 1.1947, Validation Accuracy: 0.7969\n",
      "Epoch 80, Validation Loss: 1.1916, Validation Accuracy: 0.7969\n",
      "Epoch 81, Validation Loss: 1.1859, Validation Accuracy: 0.7891\n",
      "Epoch 82, Validation Loss: 1.1873, Validation Accuracy: 0.8047\n",
      "Epoch 83, Validation Loss: 1.1864, Validation Accuracy: 0.8047\n",
      "Epoch 84, Validation Loss: 1.1836, Validation Accuracy: 0.8047\n",
      "Epoch 85, Validation Loss: 1.1873, Validation Accuracy: 0.7969\n",
      "Epoch 86, Validation Loss: 1.1790, Validation Accuracy: 0.7969\n",
      "Epoch 87, Validation Loss: 1.1743, Validation Accuracy: 0.7891\n",
      "Epoch 88, Validation Loss: 1.1780, Validation Accuracy: 0.7969\n",
      "Epoch 89, Validation Loss: 1.1752, Validation Accuracy: 0.7969\n",
      "Epoch 90, Validation Loss: 1.1723, Validation Accuracy: 0.7891\n",
      "Epoch 91, Validation Loss: 1.1796, Validation Accuracy: 0.7891\n",
      "Epoch 92, Validation Loss: 1.1768, Validation Accuracy: 0.7969\n",
      "Epoch 93, Validation Loss: 1.1679, Validation Accuracy: 0.7891\n",
      "Epoch 94, Validation Loss: 1.1727, Validation Accuracy: 0.7969\n",
      "Epoch 95, Validation Loss: 1.1707, Validation Accuracy: 0.7891\n",
      "Epoch 96, Validation Loss: 1.1647, Validation Accuracy: 0.7812\n",
      "Epoch 97, Validation Loss: 1.1641, Validation Accuracy: 0.7891\n",
      "Epoch 98, Validation Loss: 1.1653, Validation Accuracy: 0.7891\n",
      "Epoch 99, Validation Loss: 1.1693, Validation Accuracy: 0.7891\n",
      "Epoch 100, Validation Loss: 1.1592, Validation Accuracy: 0.7891\n",
      "Epoch 101, Validation Loss: 1.1597, Validation Accuracy: 0.7891\n",
      "Epoch 102, Validation Loss: 1.1638, Validation Accuracy: 0.7891\n",
      "Epoch 103, Validation Loss: 1.1564, Validation Accuracy: 0.7891\n",
      "Epoch 104, Validation Loss: 1.1549, Validation Accuracy: 0.8125\n",
      "Epoch 105, Validation Loss: 1.1529, Validation Accuracy: 0.8125\n",
      "Epoch 106, Validation Loss: 1.1493, Validation Accuracy: 0.8125\n",
      "Epoch 107, Validation Loss: 1.1491, Validation Accuracy: 0.7891\n",
      "Epoch 108, Validation Loss: 1.1502, Validation Accuracy: 0.7891\n",
      "Epoch 109, Validation Loss: 1.1509, Validation Accuracy: 0.7969\n",
      "Epoch 110, Validation Loss: 1.1457, Validation Accuracy: 0.8047\n",
      "Epoch 111, Validation Loss: 1.1498, Validation Accuracy: 0.7891\n",
      "Epoch 112, Validation Loss: 1.1469, Validation Accuracy: 0.7891\n",
      "Epoch 113, Validation Loss: 1.1434, Validation Accuracy: 0.8125\n",
      "Epoch 114, Validation Loss: 1.1440, Validation Accuracy: 0.8125\n",
      "Epoch 115, Validation Loss: 1.1387, Validation Accuracy: 0.8125\n",
      "Epoch 116, Validation Loss: 1.1378, Validation Accuracy: 0.8281\n",
      "Epoch 117, Validation Loss: 1.1385, Validation Accuracy: 0.8203\n",
      "Epoch 118, Validation Loss: 1.1393, Validation Accuracy: 0.8203\n",
      "Epoch 119, Validation Loss: 1.1356, Validation Accuracy: 0.8203\n",
      "Epoch 120, Validation Loss: 1.1339, Validation Accuracy: 0.8203\n",
      "Epoch 121, Validation Loss: 1.1366, Validation Accuracy: 0.8203\n",
      "Epoch 122, Validation Loss: 1.1346, Validation Accuracy: 0.8281\n",
      "Epoch 123, Validation Loss: 1.1315, Validation Accuracy: 0.8281\n",
      "Epoch 124, Validation Loss: 1.1311, Validation Accuracy: 0.8281\n",
      "Epoch 125, Validation Loss: 1.1309, Validation Accuracy: 0.8281\n",
      "Epoch 126, Validation Loss: 1.1312, Validation Accuracy: 0.8203\n",
      "Epoch 127, Validation Loss: 1.1302, Validation Accuracy: 0.8281\n",
      "Epoch 128, Validation Loss: 1.1281, Validation Accuracy: 0.8281\n",
      "Epoch 129, Validation Loss: 1.1284, Validation Accuracy: 0.8281\n",
      "Epoch 130, Validation Loss: 1.1281, Validation Accuracy: 0.8281\n",
      "Epoch 131, Validation Loss: 1.1253, Validation Accuracy: 0.8281\n",
      "Epoch 132, Validation Loss: 1.1287, Validation Accuracy: 0.8359\n",
      "Epoch 133, Validation Loss: 1.1202, Validation Accuracy: 0.8359\n",
      "Epoch 134, Validation Loss: 1.1203, Validation Accuracy: 0.8281\n",
      "Epoch 135, Validation Loss: 1.1238, Validation Accuracy: 0.8359\n",
      "Epoch 136, Validation Loss: 1.1191, Validation Accuracy: 0.8359\n",
      "Epoch 137, Validation Loss: 1.1186, Validation Accuracy: 0.8281\n",
      "Epoch 138, Validation Loss: 1.1167, Validation Accuracy: 0.8359\n",
      "Epoch 139, Validation Loss: 1.1147, Validation Accuracy: 0.8281\n",
      "Epoch 140, Validation Loss: 1.1160, Validation Accuracy: 0.8359\n",
      "Epoch 141, Validation Loss: 1.1153, Validation Accuracy: 0.8359\n",
      "Epoch 142, Validation Loss: 1.1142, Validation Accuracy: 0.8359\n",
      "Epoch 143, Validation Loss: 1.1123, Validation Accuracy: 0.8281\n",
      "Epoch 144, Validation Loss: 1.1102, Validation Accuracy: 0.8281\n",
      "Epoch 145, Validation Loss: 1.1087, Validation Accuracy: 0.8281\n",
      "Epoch 146, Validation Loss: 1.1100, Validation Accuracy: 0.8281\n",
      "Epoch 147, Validation Loss: 1.1105, Validation Accuracy: 0.8438\n",
      "Epoch 148, Validation Loss: 1.1068, Validation Accuracy: 0.8359\n",
      "Epoch 149, Validation Loss: 1.1070, Validation Accuracy: 0.8359\n",
      "Epoch 150, Validation Loss: 1.1078, Validation Accuracy: 0.8359\n",
      "Epoch 151, Validation Loss: 1.1060, Validation Accuracy: 0.8438\n",
      "Epoch 152, Validation Loss: 1.1098, Validation Accuracy: 0.8516\n",
      "Epoch 153, Validation Loss: 1.1029, Validation Accuracy: 0.8359\n",
      "Epoch 154, Validation Loss: 1.1026, Validation Accuracy: 0.8438\n",
      "Epoch 155, Validation Loss: 1.1033, Validation Accuracy: 0.8516\n",
      "Epoch 156, Validation Loss: 1.1015, Validation Accuracy: 0.8594\n",
      "Epoch 157, Validation Loss: 1.0992, Validation Accuracy: 0.8516\n",
      "Epoch 158, Validation Loss: 1.0984, Validation Accuracy: 0.8516\n",
      "Epoch 159, Validation Loss: 1.1015, Validation Accuracy: 0.8438\n",
      "Epoch 160, Validation Loss: 1.0954, Validation Accuracy: 0.8438\n",
      "Epoch 161, Validation Loss: 1.0969, Validation Accuracy: 0.8672\n",
      "Epoch 162, Validation Loss: 1.0935, Validation Accuracy: 0.8672\n",
      "Epoch 163, Validation Loss: 1.0941, Validation Accuracy: 0.8438\n",
      "Epoch 164, Validation Loss: 1.0937, Validation Accuracy: 0.8672\n",
      "Epoch 165, Validation Loss: 1.0928, Validation Accuracy: 0.8594\n",
      "Epoch 166, Validation Loss: 1.0945, Validation Accuracy: 0.8672\n",
      "Epoch 167, Validation Loss: 1.0910, Validation Accuracy: 0.8438\n",
      "Epoch 168, Validation Loss: 1.0920, Validation Accuracy: 0.8750\n",
      "Epoch 169, Validation Loss: 1.0921, Validation Accuracy: 0.8750\n",
      "Epoch 170, Validation Loss: 1.0881, Validation Accuracy: 0.8750\n",
      "Epoch 171, Validation Loss: 1.0908, Validation Accuracy: 0.8672\n",
      "Epoch 172, Validation Loss: 1.0907, Validation Accuracy: 0.8672\n",
      "Epoch 173, Validation Loss: 1.0889, Validation Accuracy: 0.8594\n",
      "Epoch 174, Validation Loss: 1.0901, Validation Accuracy: 0.8594\n",
      "Epoch 175, Validation Loss: 1.0877, Validation Accuracy: 0.8594\n",
      "Epoch 176, Validation Loss: 1.0873, Validation Accuracy: 0.8906\n",
      "Epoch 177, Validation Loss: 1.0875, Validation Accuracy: 0.8906\n",
      "Epoch 178, Validation Loss: 1.0835, Validation Accuracy: 0.8906\n",
      "Epoch 179, Validation Loss: 1.0886, Validation Accuracy: 0.8906\n",
      "Epoch 180, Validation Loss: 1.0852, Validation Accuracy: 0.8984\n",
      "Epoch 181, Validation Loss: 1.0889, Validation Accuracy: 0.8906\n",
      "Epoch 182, Validation Loss: 1.0825, Validation Accuracy: 0.8906\n",
      "Epoch 183, Validation Loss: 1.0843, Validation Accuracy: 0.8828\n",
      "Epoch 184, Validation Loss: 1.0847, Validation Accuracy: 0.8984\n",
      "Epoch 185, Validation Loss: 1.0826, Validation Accuracy: 0.8906\n",
      "Epoch 186, Validation Loss: 1.0799, Validation Accuracy: 0.8984\n",
      "Epoch 187, Validation Loss: 1.0770, Validation Accuracy: 0.8906\n",
      "Epoch 188, Validation Loss: 1.0791, Validation Accuracy: 0.8984\n",
      "Epoch 189, Validation Loss: 1.0780, Validation Accuracy: 0.8906\n",
      "Epoch 190, Validation Loss: 1.0812, Validation Accuracy: 0.8984\n",
      "Epoch 191, Validation Loss: 1.0792, Validation Accuracy: 0.8984\n",
      "Epoch 192, Validation Loss: 1.0767, Validation Accuracy: 0.8984\n",
      "Epoch 193, Validation Loss: 1.0788, Validation Accuracy: 0.8984\n",
      "Epoch 194, Validation Loss: 1.0800, Validation Accuracy: 0.8984\n",
      "Epoch 195, Validation Loss: 1.0784, Validation Accuracy: 0.9062\n",
      "Epoch 196, Validation Loss: 1.0762, Validation Accuracy: 0.8984\n",
      "Epoch 197, Validation Loss: 1.0751, Validation Accuracy: 0.8984\n",
      "Epoch 198, Validation Loss: 1.0731, Validation Accuracy: 0.8984\n",
      "Epoch 199, Validation Loss: 1.0727, Validation Accuracy: 0.8984\n",
      "Epoch 200, Validation Loss: 1.0721, Validation Accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "# # Define the MLP model\n",
    "    \n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_prob):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "        # Softmax activation for the output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "            x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.layers[-1](x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPClassifier(input_size, hidden_size, num_classes, dropout_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "# Function to predict the class of new data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        _, predicted_class = torch.max(output, dim=1)\n",
    "    return predicted_class\n",
    "\n",
    "# Function to compute the accuracy\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred, dim=1)  # Get the index of the max log-probability\n",
    "    correct = (predicted == y_true).float().sum()\n",
    "    return correct / y_true.shape[0]\n",
    "\n",
    "# Training and evaluation loop\n",
    "def train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # scheduler.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss += criterion(y_val_pred, y_val).item()\n",
    "                val_accuracy += calculate_accuracy(y_val_pred, y_val)\n",
    "                _, predicted_classes = torch.max(y_val_pred, dim=1)\n",
    "                # print(f'Predicted: {predicted_classes}, Actual: {y_val}') # actual predicted values\n",
    "        \n",
    "        \n",
    "        # Average the loss and accuracy over all validation batches\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Assuming the rest of your setup (model initialization, data loaders, etc.) is already done\n",
    "# Now you would just call train_and_evaluate\n",
    "train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# in_channels = params['in_channels']\n",
    "# out_channels = params['out_channels']\n",
    "# kernel_sizes = params['kernel_sizes']\n",
    "\n",
    "# class CNN1DClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_sizes, num_classes, dropout_rate=0.5):\n",
    "#         super(CNN1DClassifier, self).__init__()\n",
    "        \n",
    "#         assert len(out_channels) == len(kernel_sizes), \"The length of out_channels and kernel_sizes must be the same\"\n",
    "        \n",
    "#         self.convs = nn.ModuleList()\n",
    "#         self.bns = nn.ModuleList()  # Adding batch normalization if needed\n",
    "        \n",
    "#         current_in_channels = in_channels\n",
    "        \n",
    "#         for out_channel, kernel_size in zip(out_channels, kernel_sizes):\n",
    "#             self.convs.append(nn.Conv1d(current_in_channels, out_channel, kernel_size=kernel_size, stride=1, padding=kernel_size // 2))\n",
    "#             self.bns.append(nn.BatchNorm1d(out_channel))  # Optional: Add batch normalization\n",
    "#             current_in_channels = out_channel\n",
    "        \n",
    "#         # Calculate the size after all convolutional and pooling layers\n",
    "#         conv_output_size = input_size\n",
    "#         for kernel_size in kernel_sizes:\n",
    "#             conv_output_size = (conv_output_size + 2 * (kernel_size // 2) - (kernel_size - 1) - 1) // 1 + 1\n",
    "#             conv_output_size = conv_output_size // 2  # After pooling\n",
    "        \n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(out_channels[-1] * conv_output_size, 128)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'Input shape: {x.shape}')\n",
    "#         for conv, bn in zip(self.convs, self.bns):\n",
    "#             x = self.pool(torch.relu(bn(conv(x))))\n",
    "#             # print(f'After conv and pool: {x.shape}')\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         # print(f'After flatten: {x.shape}')\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # print(f'After fc1: {x.shape}')\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(f'After fc2: {x.shape}')\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# # Train the 1D CNN model\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, device='cpu'):\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "#         print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "#         print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_corrects = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "#                 val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#         val_loss = val_loss / len(val_loader.dataset)\n",
    "#         val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "#         print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = CNN1DClassifier(in_channels, out_channels, kernel_sizes, num_classes)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# test_corrects = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         test_loss += loss.item() * inputs.size(0)\n",
    "#         test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "# test_loss = test_loss / len(test_loader.dataset)\n",
    "# test_acc = test_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
