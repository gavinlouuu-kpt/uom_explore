{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>140</th>\n",
       "      <th>150</th>\n",
       "      <th>152</th>\n",
       "      <th>155</th>\n",
       "      <th>157</th>\n",
       "      <th>160</th>\n",
       "      <th>162</th>\n",
       "      <th>165</th>\n",
       "      <th>167</th>\n",
       "      <th>170</th>\n",
       "      <th>...</th>\n",
       "      <th>230</th>\n",
       "      <th>232</th>\n",
       "      <th>235</th>\n",
       "      <th>237</th>\n",
       "      <th>240</th>\n",
       "      <th>242</th>\n",
       "      <th>245</th>\n",
       "      <th>247</th>\n",
       "      <th>250</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289992</td>\n",
       "      <td>0.265276</td>\n",
       "      <td>0.308857</td>\n",
       "      <td>0.316474</td>\n",
       "      <td>0.301683</td>\n",
       "      <td>0.354073</td>\n",
       "      <td>0.308133</td>\n",
       "      <td>0.308787</td>\n",
       "      <td>0.253672</td>\n",
       "      <td>0.249509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351504</td>\n",
       "      <td>0.319964</td>\n",
       "      <td>0.304643</td>\n",
       "      <td>0.301340</td>\n",
       "      <td>0.315273</td>\n",
       "      <td>0.300079</td>\n",
       "      <td>0.306160</td>\n",
       "      <td>0.297775</td>\n",
       "      <td>0.302454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190347</td>\n",
       "      <td>0.201155</td>\n",
       "      <td>0.261942</td>\n",
       "      <td>0.259362</td>\n",
       "      <td>0.239436</td>\n",
       "      <td>0.214840</td>\n",
       "      <td>0.243046</td>\n",
       "      <td>0.197277</td>\n",
       "      <td>0.206084</td>\n",
       "      <td>0.218502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214454</td>\n",
       "      <td>0.215712</td>\n",
       "      <td>0.209748</td>\n",
       "      <td>0.206435</td>\n",
       "      <td>0.221081</td>\n",
       "      <td>0.231202</td>\n",
       "      <td>0.197481</td>\n",
       "      <td>0.207455</td>\n",
       "      <td>0.201596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.099843</td>\n",
       "      <td>0.100285</td>\n",
       "      <td>0.123185</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>0.265924</td>\n",
       "      <td>0.260067</td>\n",
       "      <td>0.109472</td>\n",
       "      <td>0.078298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131489</td>\n",
       "      <td>0.084056</td>\n",
       "      <td>0.108258</td>\n",
       "      <td>0.112591</td>\n",
       "      <td>0.125032</td>\n",
       "      <td>0.115088</td>\n",
       "      <td>0.111438</td>\n",
       "      <td>0.109358</td>\n",
       "      <td>0.081325</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.290731</td>\n",
       "      <td>0.266106</td>\n",
       "      <td>0.355530</td>\n",
       "      <td>0.396649</td>\n",
       "      <td>0.635214</td>\n",
       "      <td>0.552152</td>\n",
       "      <td>0.323083</td>\n",
       "      <td>0.364723</td>\n",
       "      <td>0.307880</td>\n",
       "      <td>0.288747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302109</td>\n",
       "      <td>0.309628</td>\n",
       "      <td>0.306289</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>0.311936</td>\n",
       "      <td>0.309058</td>\n",
       "      <td>0.292207</td>\n",
       "      <td>0.280615</td>\n",
       "      <td>0.278412</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.039159</td>\n",
       "      <td>6.132384</td>\n",
       "      <td>6.641051</td>\n",
       "      <td>5.897604</td>\n",
       "      <td>5.541157</td>\n",
       "      <td>5.579751</td>\n",
       "      <td>5.564905</td>\n",
       "      <td>5.541405</td>\n",
       "      <td>5.909158</td>\n",
       "      <td>5.894632</td>\n",
       "      <td>...</td>\n",
       "      <td>7.065215</td>\n",
       "      <td>6.754524</td>\n",
       "      <td>6.712956</td>\n",
       "      <td>6.570870</td>\n",
       "      <td>6.562759</td>\n",
       "      <td>6.455663</td>\n",
       "      <td>6.417492</td>\n",
       "      <td>6.324318</td>\n",
       "      <td>6.198248</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        140       150       152       155       157       160       162  \\\n",
       "0  0.289992  0.265276  0.308857  0.316474  0.301683  0.354073  0.308133   \n",
       "1  0.190347  0.201155  0.261942  0.259362  0.239436  0.214840  0.243046   \n",
       "2  0.099843  0.100285  0.123185  0.163675  0.353439  0.310796  0.265924   \n",
       "3  0.290731  0.266106  0.355530  0.396649  0.635214  0.552152  0.323083   \n",
       "4  6.039159  6.132384  6.641051  5.897604  5.541157  5.579751  5.564905   \n",
       "\n",
       "        165       167       170  ...       230       232       235       237  \\\n",
       "0  0.308787  0.253672  0.249509  ...  0.351504  0.319964  0.304643  0.301340   \n",
       "1  0.197277  0.206084  0.218502  ...  0.214454  0.215712  0.209748  0.206435   \n",
       "2  0.260067  0.109472  0.078298  ...  0.131489  0.084056  0.108258  0.112591   \n",
       "3  0.364723  0.307880  0.288747  ...  0.302109  0.309628  0.306289  0.307220   \n",
       "4  5.541405  5.909158  5.894632  ...  7.065215  6.754524  6.712956  6.570870   \n",
       "\n",
       "        240       242       245       247       250  channel_id  \n",
       "0  0.315273  0.300079  0.306160  0.297775  0.302454           0  \n",
       "1  0.221081  0.231202  0.197481  0.207455  0.201596           1  \n",
       "2  0.125032  0.115088  0.111438  0.109358  0.081325           2  \n",
       "3  0.311936  0.309058  0.292207  0.280615  0.278412           3  \n",
       "4  6.562759  6.455663  6.417492  6.324318  6.198248           4  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/gavinlouuu/coding/uom_explore/model_input/feature_matrix.csv\"\n",
    "param_path = \"/home/gavinlouuu/coding/uom_explore/data_science/parameter.json\"\n",
    "\n",
    "with open('parameter.json','r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "# drop experiment_id column\n",
    "df.drop('experiment_id', axis=1, inplace=True)\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Extract parameters from the JSON object\n",
    "hidden_size = params['hidden_size']\n",
    "ground_truth = params['ground_truth']\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "learning_rate = params['learning_rate']\n",
    "momentum_value = params['momentum_value']\n",
    "dropout_rate = params['dropout']\n",
    "\n",
    "input_size = len(df.columns)-1  # removing the ground truth from the number of columns counted\n",
    "num_classes = df[ground_truth].nunique()\n",
    "\n",
    "# Replace 'target' with the name of your actual target column\n",
    "X = df.drop(ground_truth, axis=1)\n",
    "y = df[ground_truth]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # This makes 60%, 20%, 20%\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(0,255)) # \n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(1)  # Shape: [batch_size, 1, num_features]\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)  # Convert to NumPy array first\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_scaled, y_train)\n",
    "val_dataset = TensorDataset(X_val_scaled, y_val)\n",
    "test_dataset = TensorDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Define the MLP model\n",
    "    \n",
    "# class MLPClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_sizes, num_classes, dropout_prob):\n",
    "#         super(MLPClassifier, self).__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.batch_norms = nn.ModuleList()\n",
    "#         self.dropout_prob = dropout_prob\n",
    "        \n",
    "#         # Input layer\n",
    "#         self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "#         self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "\n",
    "#         # Hidden layers\n",
    "#         for i in range(len(hidden_sizes) - 1):\n",
    "#             self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "#             self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        \n",
    "#         # Softmax activation for the output layer\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         for i in range(len(self.layers) - 1):\n",
    "#             x = torch.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "#             x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "#         x = self.layers[-1](x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the model\n",
    "# model = MLPClassifier(input_size, hidden_size, num_classes, dropout_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "# # Function to predict the class of new data\n",
    "# def predict(model, data):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         output = model(data)\n",
    "#         _, predicted_class = torch.max(output, dim=1)\n",
    "#     return predicted_class\n",
    "\n",
    "# # Function to compute the accuracy\n",
    "# def calculate_accuracy(y_pred, y_true):\n",
    "#     _, predicted = torch.max(y_pred, dim=1)  # Get the index of the max log-probability\n",
    "#     correct = (predicted == y_true).float().sum()\n",
    "#     return correct / y_true.shape[0]\n",
    "\n",
    "# # Training and evaluation loop\n",
    "# def train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         # scheduler.step()\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_accuracy = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X_val, y_val in val_loader:\n",
    "#                 y_val_pred = model(X_val)\n",
    "#                 val_loss += criterion(y_val_pred, y_val).item()\n",
    "#                 val_accuracy += calculate_accuracy(y_val_pred, y_val)\n",
    "#                 _, predicted_classes = torch.max(y_val_pred, dim=1)\n",
    "#                 # print(f'Predicted: {predicted_classes}, Actual: {y_val}') # actual predicted values\n",
    "        \n",
    "        \n",
    "#         # Average the loss and accuracy over all validation batches\n",
    "#         val_loss /= len(val_loader)\n",
    "#         val_accuracy /= len(val_loader)\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# # Assuming the rest of your setup (model initialization, data loaders, etc.) is already done\n",
    "# # Now you would just call train_and_evaluate\n",
    "# train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6190\n",
      "Test Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "out_s1 = params['out_channels1']\n",
    "out_s2 = params['out_channels2']\n",
    "kernel_s1 = params['kernel_size1']\n",
    "kernel_s2 = params['kernel_size2']\n",
    "\n",
    "class CNN1DClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, num_classes, dropout_rate=0.5):\n",
    "        super(CNN1DClassifier, self).__init__()\n",
    "        \n",
    "        assert len(out_channels) == len(kernel_sizes), \"The length of out_channels and kernel_sizes must be the same\"\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()  # Adding batch normalization if needed\n",
    "        \n",
    "        current_in_channels = in_channels\n",
    "        \n",
    "        for out_channel, kernel_size in zip(out_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(current_in_channels, out_channel, kernel_size=kernel_size, stride=1, padding=kernel_size // 2))\n",
    "            self.bns.append(nn.BatchNorm1d(out_channel))  # Optional: Add batch normalization\n",
    "            current_in_channels = out_channel\n",
    "        \n",
    "        # Calculate the size after all convolutional and pooling layers\n",
    "        conv_output_size = input_size\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv_output_size = (conv_output_size + 2 * (kernel_size // 2) - (kernel_size - 1) - 1) // 1 + 1\n",
    "            conv_output_size = conv_output_size // 2  # After pooling\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(out_channels[-1] * conv_output_size, 128)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'Input shape: {x.shape}')\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = self.pool(torch.relu(bn(conv(x))))\n",
    "            print(f'After conv and pool: {x.shape}')\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        print(f'After flatten: {x.shape}')\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        print(f'After fc1: {x.shape}')\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        print(f'After fc2: {x.shape}')\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "in_channels = 1\n",
    "out_channels = [128, 256, 256, 128]\n",
    "kernel_sizes = [3, 3, 3, 3]\n",
    "num_classes = df[ground_truth].nunique()\n",
    "model = CNN1DClassifier(in_channels, out_channels, kernel_sizes, num_classes)\n",
    "\n",
    "# Print model to verify\n",
    "print(model)\n",
    "\n",
    "\n",
    "# class CNN1DClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes, out_channels1=out_s1, out_channels2=out_s2, kernel_size1=kernel_s1, kernel_size2=kernel_s2, dropout_rate=dropout_rate):\n",
    "#         super(CNN1DClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels1, kernel_size=kernel_size1, stride=1, padding=kernel_size1 // 2)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=out_channels1, out_channels=out_channels2, kernel_size=kernel_size2, stride=1, padding=kernel_size2 // 2)\n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "#         # Calculate the size after two pooling layers\n",
    "#         conv_output_size = input_size\n",
    "#         conv_output_size = (conv_output_size + 2 * (kernel_size1 // 2) - (kernel_size1 - 1) - 1) // 1 + 1\n",
    "#         conv_output_size = conv_output_size // 2  # First pooling layer\n",
    "#         conv_output_size = (conv_output_size + 2 * (kernel_size2 // 2) - (kernel_size2 - 1) - 1) // 1 + 1\n",
    "#         conv_output_size = conv_output_size // 2  # Second pooling layer\n",
    "        \n",
    "#         self.fc1 = nn.Linear(out_channels2 * conv_output_size, 128)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f'Input shape: {x.shape}')\n",
    "#         x = self.pool(torch.relu(self.conv1(x)))\n",
    "#         # print(f'After conv1 and pool1: {x.shape}')\n",
    "#         x = self.pool(torch.relu(self.conv2(x)))\n",
    "#         # print(f'After conv2 and pool2: {x.shape}')\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         # print(f'After flatten: {x.shape}')\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # print(f'After fc1: {x.shape}')\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(f'After fc2: {x.shape}')\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "in_channels = 1  # Since each row is a single channel of features\n",
    "\n",
    "# Example usage\n",
    "model = CNN1DClassifier(in_channels = in_channels, out_channels1=32, out_channels2=64, kernel_size1=3, kernel_size2=3, num_classes = num_classes, dropout_rate=dropout_rate)\n",
    "\n",
    "def train_single_model(model, learning_rate, epochs, train_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the 1D CNN model\n",
    "model = CNN1DClassifier(in_channels, num_classes)\n",
    "model = train_single_model(model, learning_rate, num_epochs, train_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the 1D CNN model on the validation set\n",
    "val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Evaluate the 1D CNN model on the test set\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to train a single model\n",
    "# def train_single_model(input_size, hidden_sizes, num_classes, dropout_prob, learning_rate, epochs, train_loader):\n",
    "#     model = MLPClassifier(input_size, hidden_sizes, num_classes, dropout_prob)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for X_batch, y_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Train multiple models\n",
    "# num_models = 10  # Number of models to train\n",
    "# models = []\n",
    "# for _ in range(num_models):\n",
    "#     model = train_single_model(input_size, hidden_size, num_classes, dropout_rate, learning_rate, num_epochs, train_loader)\n",
    "#     models.append(model)\n",
    "\n",
    "# # Function to combine predictions from multiple models\n",
    "# def ensemble_predict(models, data_loader):\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "#     for data, labels in data_loader:\n",
    "#         model_outputs = [model(data).unsqueeze(0) for model in models]\n",
    "#         avg_outputs = torch.cat(model_outputs, dim=0).mean(dim=0)\n",
    "#         _, predicted_class = torch.max(avg_outputs, dim=1)\n",
    "#         all_predictions.append(predicted_class)\n",
    "#         all_labels.append(labels)\n",
    "    \n",
    "#     all_predictions = torch.cat(all_predictions)\n",
    "#     all_labels = torch.cat(all_labels)\n",
    "    \n",
    "#     return all_predictions, all_labels\n",
    "\n",
    "# # Function to compute accuracy\n",
    "# def calculate_accuracy(y_pred, y_true):\n",
    "#     correct = (y_pred == y_true).float().sum()\n",
    "#     return correct / y_true.shape[0]\n",
    "\n",
    "# # Evaluate ensemble on validation set\n",
    "# predicted_classes, true_classes = ensemble_predict(models, val_loader)\n",
    "# accuracy = calculate_accuracy(predicted_classes, true_classes)\n",
    "# print(f'Ensemble Validation Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
